
@incollection{NIPS2017_7195,
title = {Implicit Regularization in Matrix Factorization},
author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6151--6159},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization.pdf}
}
@article{doi:10.1080/02331934.2021.2009828,
author = {Hedy Attouch and Zaki Chbani and Jalal Fadili and Hassan Riahi},
title = {Convergence of iterates for first-order optimization algorithms with inertia and Hessian driven damping},
journal = {Optimization},
volume = {0},
number = {0},
pages = {1-40},
year  = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/02331934.2021.2009828},

URL = { 
    
        https://doi.org/10.1080/02331934.2021.2009828
    
    

},
eprint = { 
    
        https://doi.org/10.1080/02331934.2021.2009828
    
    

}

}
@article{li2020accelerated,
  title={Accelerated first-order optimization algorithms for machine learning},
  author={Li, Huan and Fang, Cong and Lin, Zhouchen},
  journal={Proceedings of the IEEE},
  volume={108},
  number={11},
  pages={2067--2082},
  year={2020},
  publisher={IEEE}
}
@ARTICLE{8979360,  author={Chen, Ruijuan and Li, Xiuting},  journal={IEEE Access},   title={Implicit Runge-Kutta Methods for Accelerated Unconstrained Convex Optimization},   year={2020},  volume={8},  number={},  pages={28624-28634},  doi={10.1109/ACCESS.2020.2967064}}
@article{chen2022gradient,
  title={Gradient Norm Minimization of Nesterov Acceleration: $ o (1/k^{3} ) $},
  author={Chen, Shuo and Shi, Bin and Yuan, Ya-xiang},
  journal={arXiv preprint arXiv:2209.08862},
  year={2022}
}
@ARTICLE{7967721,
  author={Van Scoy, Bryan and Freeman, Randy A. and Lynch, Kevin M.},
  journal={IEEE Control Systems Letters}, 
  title={The Fastest Known Globally Convergent First-Order Method for Minimizing Strongly Convex Functions}, 
  year={2018},
  volume={2},
  number={1},
  pages={49-54},
  doi={10.1109/LCSYS.2017.2722406}}
@article{beck2009fast,
  title={A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
  author={Beck, Amir and Teboulle, Marc},
  journal={SIAM journal on imaging sciences},
  volume={2},
  number={1},
  pages={183--202},
  year={2009},
  publisher={SIAM}
}
@inproceedings{DBLP:conf/cdc/SunGK20,
  author    = {Boya Sun and
               Jemin George and
               Solmaz S. Kia},
  title     = {High-Resolution Modeling of the Fastest First-Order Optimization Method
               for Strongly Convex Functions},
  booktitle = {59th {IEEE} Conference on Decision and Control, {CDC} 2020, Jeju Island,
               South Korea, December 14-18, 2020},
  pages     = {4237--4242},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/CDC42340.2020.9304444},
  doi       = {10.1109/CDC42340.2020.9304444},
  timestamp = {Fri, 04 Mar 2022 13:31:02 +0100},
  biburl    = {https://dblp.org/rec/conf/cdc/SunGK20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{gel1961principle,
  title={The principle of nonlocal search in automatic optimization systems},
  author={Gel'fand, Izrail Moiseevich and Tsetlin, Mikhail L'vovich},
  booktitle={Doklady Akademii Nauk},
  volume={137},
  number={2},
  pages={295--298},
  year={1961},
  organization={Russian Academy of Sciences}
}
@article{attouch2021convergence,
  title={Convergence of iterates for first-order optimization algorithms with inertia and Hessian driven damping},
  author={Attouch, H{\'e}dy and Chbani, Zaki and Fadili, Jalal and Riahi, Hassan},
  journal={Optimization},
  pages={1--40},
  year={2021},
  publisher={Taylor \& Francis}
}
@article{siegel2019accelerated,
  title={Accelerated first-order methods: Differential equations and Lyapunov functions},
  author={Siegel, Jonathan W},
  journal={arXiv preprint arXiv:1903.05671},
  year={2019}
}
@article{attouch2020first,
  title={First-order optimization algorithms via inertial systems with Hessian driven damping},
  author={Attouch, Hedy and Chbani, Zaki and Fadili, Jalal and Riahi, Hassan},
  journal={Mathematical Programming},
  pages={1--43},
  year={2020},
  publisher={Springer}
}
@inproceedings{jin2018accelerated,
  title={Accelerated gradient descent escapes saddle points faster than gradient descent},
  author={Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I},
  booktitle={Conference On Learning Theory},
  pages={1042--1085},
  year={2018},
  organization={PMLR}
}
@inproceedings{lee2016gradient,
  title={Gradient descent only converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  booktitle={Conference on learning theory},
  pages={1246--1257},
  year={2016},
  organization={PMLR}
}
@article{nesterov2008accelerating,
  title={Accelerating the cubic regularization of Newton’s method on convex problems},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={112},
  number={1},
  pages={159--181},
  year={2008},
  publisher={Springer}
}
@article{chen2022accelerating,
  title={Accelerating adaptive cubic regularization of Newton’s method via random sampling},
  author={Chen, Xi and Jiang, Bo and Lin, Tianyi and Zhang, Shuzhong},
  journal={J. Mach. Learn. Res},
  year={2022}
}
@article{Nesterov1983AMF,
  title={A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
  author={Y. Nesterov},
  journal={Proceedings of the USSR Academy of Sciences},
  year={1983},
  volume={269},
  pages={543-547}
}
@article{muehlebach2021optimization,
  title={Optimization with Momentum: Dynamical, Control-Theoretic, and Symplectic Perspectives.},
  author={Muehlebach, Michael and Jordan, Michael I},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={73},
  pages={1--50},
  year={2021}
}
@inproceedings{Zhang2018DirectRD,
  title={Direct Runge-Kutta Discretization Achieves Acceleration},
  author={J. Zhang and Aryan Mokhtari and Suvrit Sra and Ali Jadbabaie},
  booktitle={NeurIPS},
  year={2018}
}
@article{Devolder2014FirstorderMO,
  title={First-order methods of smooth convex optimization with inexact oracle},
  author={Olivier Devolder and François Glineur and Yurii Nesterov},
  journal={Mathematical Programming},
  year={2014},
  volume={146},
  pages={37-75}
}
@article{Lessard2016AnalysisAD,
  title={Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints},
  author={Laurent Lessard and Benjamin Recht and Andrew Packard},
  journal={SIAM J. Optim.},
  year={2016},
  volume={26},
  pages={57-95}
}
@article{Shi2021UnderstandingTA,
  title={Understanding the Acceleration Phenomenon via High-Resolution Differential Equations},
  author={Bin Shi and Simon Shaolei Du and Michael I. Jordan and Weijie J. Su},
  journal={ArXiv},
  year={2021},
  volume={abs/1810.08907}
}
@article{shi2019acceleration,
  title={Acceleration via symplectic discretization of high-resolution differential equations},
  author={Shi, Bin and Du, Simon S and Su, Weijie J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1902.03694},
  year={2019}
}
@inproceedings{zhang2021revisiting,
  title={Revisiting the Role of Euler Numerical Integration on Acceleration and Stability in Convex Optimization},
  author={Zhang, Peiyuan and Orvieto, Antonio and Daneshmand, Hadi and Hofmann, Thomas and Smith, Roy S},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3979--3987},
  year={2021},
  organization={PMLR}
}
@article{OPT-036,
url = {http://dx.doi.org/10.1561/2400000036},
year = {2021},
volume = {5},
journal = {Foundations and Trends® in Optimization},
title = {Acceleration Methods},
doi = {10.1561/2400000036},
issn = {2167-3888},
number = {1-2},
pages = {1-245},
author = {Alexandre d’Aspremont and Damien Scieur and Adrien Taylor}
}
@article {WibisonoE7351,
	author = {Wibisono, Andre and Wilson, Ashia C. and Jordan, Michael I.},
	title = {A variational perspective on accelerated methods in optimization},
	volume = {113},
	number = {47},
	pages = {E7351--E7358},
	year = {2016},
	doi = {10.1073/pnas.1614734113},
	publisher = {National Academy of Sciences},
	abstract = {Optimization problems arise naturally in statistical machine learning and other fields concerned with data analysis. The rapid growth in the scale and complexity of modern datasets has led to a focus on gradient-based methods and also on the class of accelerated methods, first proposed by Nesterov in 1983. Accelerated methods achieve faster convergence rates than gradient methods and indeed, under certain conditions, they achieve optimal rates. However, accelerated methods are not descent methods and remain a conceptual mystery. We propose a variational, continuous-time framework for understanding accelerated methods. We provide a systematic methodology for converting accelerated higher-order methods from continuous time to discrete time. Our work illuminates a class of dynamics that may be useful for designing better algorithms for optimization.Accelerated gradient methods play a central role in optimization, achieving optimal rates in many settings. Although many generalizations and extensions of Nesterov{\textquoteright}s original acceleration method have been proposed, it is not yet clear what is the natural scope of the acceleration concept. In this paper, we study accelerated methods from a continuous-time perspective. We show that there is a Lagrangian functional that we call the Bregman Lagrangian, which generates a large class of accelerated methods in continuous time, including (but not limited to) accelerated gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods. We show that the continuous-time limit of all of these methods corresponds to traveling the same curve in spacetime at different speeds. From this perspective, Nesterov{\textquoteright}s technique and many of its generalizations can be viewed as a systematic way to go from the continuous-time curves generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/113/47/E7351},
	eprint = {https://www.pnas.org/content/113/47/E7351.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}
@article{JMLR:v17:15-084,
  author  = {Weijie Su and Stephen Boyd and Emmanuel J. Cand{{\`e}}s},
  title   = {A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {153},
  pages   = {1-43},
  url     = {http://jmlr.org/papers/v17/15-084.html}
}
@InProceedings{pmlr-v49-lee16,
  title = 	 {Gradient Descent Only Converges to Minimizers},
  author = 	 {Jason D. Lee and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
  pages = 	 {1246--1257},
  year = 	 {2016},
  editor = 	 {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v49/lee16.pdf},
  url = 	 {http://proceedings.mlr.press/v49/lee16.html},
  abstract = 	 {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.}
}


@article{sanz2021connections,
  title={The connections between Lyapunov functions for some optimization algorithms and differential equations},
  author={Sanz Serna, Jes{\'u}s Mar{\'\i}a and Zygalakis, Konstantinos C},
  journal={SIAM Journal on Numerical Analysis},
  volume={59},
  number={3},
  pages={1542--1565},
  year={2021},
  publisher={SIAM}
}
@article{khalil2002nonlinear,
  title={Nonlinear systems third edition},
  author={Khalil, Hassan K},
  journal={Patience Hall},
  volume={115},
  year={2002}
}
@article{doi:10.1137/17M1136845,
author = {Fazlyab, Mahyar and Ribeiro, Alejandro and Morari, Manfred and Preciado, Victor M.},
title = {Analysis of Optimization Algorithms via Integral Quadratic Constraints: Nonstrongly Convex Problems},
journal = {SIAM Journal on Optimization},
volume = {28},
number = {3},
pages = {2654-2689},
year = {2018},
doi = {10.1137/17M1136845},

URL = { 
        https://doi.org/10.1137/17M1136845
    
},
eprint = { 
        https://doi.org/10.1137/17M1136845
    
}
,
    abstract = { In this paper, we develop a unified framework capable of certifying both exponential and subexponential convergence rates for a wide range of iterative first-order optimization algorithms. To this end, we construct a family of parameter-dependent nonquadratic Lyapunov functions that can generate convergence rates in addition to proving asymptotic convergence. Using integral quadratic constraints (IQCs) from robust control theory, we propose a linear matrix inequality (LMI) to guide the search for the parameters of the Lyapunov function in order to establish a rate bound. Based on this result, we develop a semidefinite programming (SDP) framework whose solution yields the best convergence rate that can be certified by the class of Lyapunov functions under consideration. We illustrate the utility of our results by analyzing the gradient method, proximal algorithms, and their accelerated variants for (strongly) convex problems. We also develop the continuous-time counterpart, whereby we analyze the gradient flow and the continuous-time limit of Nesterov's accelerated method. }
}
@article{sanz1992symplectic,
  title={Symplectic integrators for Hamiltonian problems: an overview},
  author={Sanz-Serna, Jesus M},
  journal={Acta numerica},
  volume={1},
  pages={243--286},
  year={1992},
  publisher={Cambridge University Press}
}
@article{hairer2006geometric,
  title={Geometric numerical integration},
  author={Hairer, Ernst and Hochbruck, Marlis and Iserles, Arieh and Lubich, Christian},
  journal={Oberwolfach Reports},
  volume={3},
  number={1},
  pages={805--882},
  year={2006}
}
@article{muehlebach2023accelerated,
  title={Accelerated First-Order Optimization under Nonlinear Constraints},
  author={Muehlebach, Michael and Jordan, Michael I},
  journal={arXiv preprint arXiv:2302.00316},
  year={2023}
}
@article{frank1956algorithm,
  title={An algorithm for quadratic programming},
  author={Frank, Marguerite and Wolfe, Philip},
  journal={Naval research logistics quarterly},
  volume={3},
  number={1-2},
  pages={95--110},
  year={1956},
  publisher={Wiley Online Library}
}
@article{baes2009estimate,
  title={Estimate sequence methods: extensions and approximations},
  author={Baes, Michel},
  journal={Institute for Operations Research, ETH, Z{\"u}rich, Switzerland},
  volume={2},
  number={1},
  year={2009}
}
@article{nesterov2015universal,
  title={Universal gradient methods for convex optimization problems},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={152},
  number={1-2},
  pages={381--404},
  year={2015},
  publisher={Springer}
}
@article{even2021continuized,
  title={A continuized view on Nesterov acceleration for stochastic gradient descent and randomized gossip},
  author={Even, Mathieu and Berthier, Rapha{\"e}l and Bach, Francis and Flammarion, Nicolas and Gaillard, Pierre and Hendrikx, Hadrien and Massouli{\'e}, Laurent and Taylor, Adrien},
  journal={arXiv preprint arXiv:2106.07644},
  year={2021}
}
@INPROCEEDINGS{7963773,
  author={Fazlyab, Mahyar and Koppel, Alec and Preciado, Victor M. and Ribeiro, Alejandro},
  booktitle={2017 American Control Conference (ACC)}, 
  title={A variational approach to dual methods for constrained convex optimization}, 
  year={2017},
  volume={},
  number={},
  pages={5269-5275},
  doi={10.23919/ACC.2017.7963773}}
@article{zhang2021rethinking,
  title={Rethinking the Variational Interpretation of Accelerated Optimization Methods},
  author={Zhang, Peiyuan and Orvieto, Antonio and Daneshmand, Hadi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14396--14406},
  year={2021}
}
@inproceedings{muehlebach2019dynamical,
  title={A dynamical systems perspective on Nesterov acceleration},
  author={Muehlebach, Michael and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={4656--4662},
  year={2019},
  organization={PMLR}
}
@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}


@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@article{doi:10.1137/20M1364138,
author = {Sanz Serna, J. M. and Zygalakis, K. C.},
title = {The Connections Between Lyapunov Functions for Some Optimization Algorithms and Differential Equations},
journal = {SIAM Journal on Numerical Analysis},
volume = {59},
number = {3},
pages = {1542-1565},
year = {2021},
doi = {10.1137/20M1364138},

URL = { 
        https://doi.org/10.1137/20M1364138
    
},
eprint = { 
        https://doi.org/10.1137/20M1364138
    
}
,
    abstract = { In this manuscript we study the properties of a family of a second-order differential equations with damping, its discretizations, and their connections with accelerated optimization algorithms for \$m\$-strongly convex and \$L\$-smooth functions. In particular, using the linear matrix inequality (LMI) framework developed by Fazlyab et. al. (2018), we derive analytically a (discrete) Lyapunov function for a two-parameter family of Nesterov optimization methods, which allows for the complete characterization of their convergence rate. In the appropriate limit, this family of methods may be seen as a discretization of a family of second-order ODEs for which we construct (continuous) Lyapunov functions by means of the LMI framework. The continuous Lyapunov functions may alternatively be obtained by studying the limiting behavior of their discrete counterparts. Finally, we show that the majority of typical discretizations of the of the family of ODEs, such as the heavy ball method, do not possess Lyapunov functions with properties similar to those of the Lyapunov function constructed here for the Nesterov method. }
}

@article{attouch2000heavy,
  title={The heavy ball with friction method, I. The continuous dynamical system: global exploration of the local minima of a real-valued function by asymptotic analysis of a dissipative dynamical system},
  author={Attouch, Hedy and Goudou, Xavier and Redont, Patrick},
  journal={Communications in Contemporary Mathematics},
  volume={2},
  number={01},
  pages={1--34},
  year={2000},
  publisher={World Scientific}
}
@article{krichene2015accelerated,
  title={Accelerated mirror descent in continuous and discrete time},
  author={Krichene, Walid and Bayen, Alexandre and Bartlett, Peter L},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{wilson2021lyapunov,
  title={A Lyapunov Analysis of Accelerated Methods in Optimization.},
  author={Wilson, Ashia C and Recht, Ben and Jordan, Michael I},
  journal={J. Mach. Learn. Res.},
  volume={22},
  pages={113--1},
  year={2021}
}
@article{chen2021unified,
  title={A Unified Convergence Analysis of First Order Convex Optimization Methods via Strong Lyapunov Functions},
  author={Chen, Long and Luo, Hao},
  journal={arXiv preprint arXiv:2108.00132},
  year={2021}
}
@inproceedings{
ma2018quasihyperbolic,
title={Quasi-hyperbolic momentum and Adam for deep learning},
author={Jerry Ma and Denis Yarats},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1fUpoR5FQ},
}
@article{campos2021discrete,
  title={A Discrete Variational Derivation of Accelerated Methods in Optimization},
  author={Campos, C{\'e}dric M and Mahillo, Alejandro and de Diego, David Mart{\'\i}n},
  journal={arXiv preprint arXiv:2106.02700},
  year={2021}
}
@article{Polyak1963GradientMF,
  title={Gradient methods for the minimisation of functionals},
  author={Boris Polyak},
  journal={Ussr Computational Mathematics and Mathematical Physics},
  year={1963},
  volume={3},
  pages={864-878}
}
@article{Lee2016GradientDC,
  title={Gradient Descent Converges to Minimizers},
  author={J. Lee and Max Simchowitz and Michael I. Jordan and B. Recht},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.04915}
}
@inproceedings{Chizat2019OnLT,
  title={On Lazy Training in Differentiable Programming},
  author={Lenaic Chizat and Edouard Oyallon and Francis Bach},
  booktitle={NeurIPS},
  year={2019}
}
@inproceedings{10.5555/2969033.2969107,
author = {Su, Weijie and Boyd, Stephen and Cand\`{e}s, Emmanuel J.},
title = {A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We derive a second-order ordinary differential equation (ODE), which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2510–2518},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{Zhang2019FastCO,
  title={Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks},
  author={Guodong Zhang and James Martens and Roger Baker Grosse},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.10961}
}
@article{10.1137/080731359,
author = {Journ\'{e}e, M. and Bach, F. and Absil, P.-A. and Sepulchre, R.},
title = {Low-Rank Optimization on the Cone of Positive Semidefinite Matrices},
year = {2010},
issue_date = {April 2010},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {20},
number = {5},
issn = {1052-6234},
url = {https://doi.org/10.1137/080731359},
doi = {10.1137/080731359},
journal = {SIAM J. on Optimization},
month = may,
pages = {2327–2351},
numpages = {25},
keywords = {large-scale algorithms, sparse principal component analysis, Riemannian quotient manifold, low-rank constraints, cone of symmetric positive definite matrices, maximum-cut algorithms}
}
@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}
@article{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1901.08584},
  year={2019}
}
@inproceedings{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Advances in neural information processing systems},
  pages={3036--3046},
  year={2018}
}
@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}
@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}

@InProceedings{pmlr-v108-laborde20a,
  title = 	 {A Lyapunov analysis for accelerated gradient methods: from deterministic to stochastic case},
  author =       {Laborde, Maxime and Oberman, Adam},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {602--612},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/laborde20a/laborde20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/laborde20a.html},
  abstract = 	 {Recent work by Su, Boyd and Candes made a connection between Nesterov’s accelerated gradient descent method and an ordinary differential equation (ODE). We show that this connection can be extended to the case of stochastic gradients, and develop Lyapunov function based convergence rates proof for Nesterov’s accelerated stochastic gradient descent. In the gradient case, we show Nesterov’s method arises as a straightforward discretization of a modified ODE. Established Lyapunov analysis is used to recover the accelerated rates of convergence in both continuous and discrete time. Moreover, the Lyapunov analysis can be extended to the case of stochastic gradients. The result is a unified approach to accelerationin both continuous and discrete time, and in for both stochastic and full gradients.}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}
@InProceedings{pmlr-v97-allen-zhu19a,
  title = 	 {A Convergence Theory for Deep Learning via Over-Parameterization},
  author = 	 {Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {242--252},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/allen-zhu19a/allen-zhu19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/allen-zhu19a.html},
  abstract = 	 {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps   e^{-T}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).}
}