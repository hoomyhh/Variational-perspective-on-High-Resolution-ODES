\documentclass{article}
\usepackage[final]{neurips_2022}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{comment}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amssymb,amsthm,amsfonts,mathrsfs}
\usepackage{graphicx}
\usepackage{dsfont,enumitem,stackrel}
\usepackage{color}
\usepackage{mathtools,amssymb,bm}
\usepackage{amstext}
\usepackage{array}
\usepackage[ruled]{algorithm}
\usepackage{algorithmic}
\usepackage{cases}
\usepackage{pgfplots}
%\usepackage{accents}
\usepackage{caption}
\usepackage{float}
\usepackage{standalone}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\C{ \mathbb{C}}
\def\R{\mathbb{R}}
\def\Z{ \mathbb{Z}}
\def\N{ \mathbb{N}}
\def\change{black}
\newcommand{\hcm}[1]{\textcolor{blue}{#1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\norm}[1]{\left\|#1\right\|}
\theoremstyle{plain}
\newtheorem{thm}{\textbf{Theorem}}
\newtheorem{lem}{\textbf{Lemma}}
\newtheorem{prop}{\textbf{Proposition}}
\newtheorem*{corl}{\textbf{Corollary}}
\theoremstyle{definition}
\newtheorem{defn}{\textbf{Definition}}
\newtheorem{conj}{\textbf{Conjecture}}
\newtheorem{example}{\textbf{Example}}
\theoremstyle{remark}
\newtheorem*{rem}{\bf Remark}
\newtheorem*{sketch}{\bf Proof sketch }
\newtheorem*{note}{Note}
\newcommand*{\rom}[1]
{\expandafter\@slowromancap\romannumeral #1@}
\title{Updates on Meeting with Kostas}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  % examples of more authors
%   Hoomaan Maskan \\
%   Department of Mathematics and Mathematical Statistics,
%        Umeå University \\
%  \texttt{Hoomaan.maskan@umu.se} \\
%   Armin Eftekhari\\
%        Department of Mathematics and Mathematical Statistics,
%        Umeå University \\
%        \texttt{armin.eftekhari@umu.se} \\
%   \And
%   Hoomaan Maskan \\
%   Department of Mathematics and Mathematical Statistics,
%        Umeå University \\
%  \texttt{Hoomaan.maskan@umu.se} \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\title{Variational Perspective on High-Resolution ODEs}
\author{Hoomaan Maskan, Alp Yurtsever, Konstantinos Zygalakis}
\date{January 2023}

\begin{document}

\maketitle
\begin{abstract}
    Machine Learning - algorithms - faster - perspectives - Variational perspective - High resolution- Gap: no high res variational perspective
\end{abstract}
\section{Introduction}\label{sec_introduction}
Optimization algorithms play an important role in big data analysis \cite{li2020accelerated} and machine learning \cite{OPT-036,wilson2021lyapunov,WibisonoE7351}. Due to huge load of data, accelerated methods in optimization are important and necessary to study \cite{Shi2021UnderstandingTA,JMLR:v17:15-084,wilson2021lyapunov,Lessard2016AnalysisAD}. \par
Consider the following unconstrained minimization problem
\begin{align}\label{problem}
    \min_{x\in \mathbb{R}^n} f(x)
\end{align}

where $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is convex and continuously differentiable with Lipschitz gradients.
The simplest algorithm to solve this problem is Gradient Descent (GD)
\begin{align}\label{gradient_descent}
    x_{k+1}=x_k - \frac{1}{L} \nabla f(x_k).\tag{GD}
\end{align}
For $x^*=\arg \min_{x\in \mathbb{R}^n}f(x)$ and step-size $1/L$, the updates of (\ref{gradient_descent}) will result in $f(x)-f(x^*)=\mathcal{O}(1/k)$.
\cite{nesterov2003introductory}. Many have tried to improve this rate using only first-order data. Nesterov proposed the globally convergent method \cite{Nesterov1983AMF}
\begin{align}\label{eqn_Nest_alg}
    y_{k+1}&=x_k -s\nabla f(x_k),\nonumber\\
    x_{k+1}&= y_{k+1}+\frac{k}{k+3} (y_{k+1}-y_{k}), \tag{NAG}
\end{align}
with $x_0=y_0\in \mathbb{R}^n$ and $0<s\leq 1/L$. The NAG method achieved a convergence rate of $f(x)-f(x^*)=\mathcal{O}(1/sk^2)$ which is noticeably faster than the GD's convergence rate. \par
Nesterov used \textit{estimate sequence} technique to prove the convergence of the NAG method. Due to the complexity of this technique, the essence of acceleration remained a mystery. Many have tried to provide a better understanding of momentum-based acceleration through different perspectives. For example \cite{JMLR:v17:15-084} proposed a continuous-time perspective (this work belongs to this group), \cite{doi:10.1137/17M1136845} uses integral quadratic constraints and control systems with non-linear feedbacks, \cite{muehlebach2019dynamical} presents a dynamical perspective, and \cite{doi:10.1080/02331934.2021.2009828} uses inertial dynamic involving both viscous damping and Hessian-driven damping. \par
Recently, Su et al. proposed a continuous-time perspective toward understanding the acceleration phenomenon \cite{JMLR:v17:15-084}. By analysing the NAG in continuous-time, they showed that the NAG method follows similar trajectories as the Ordinary Differential Equation(ODE) 
\begin{align}\label{low-res-ode}
    \Ddot{X}_t+\frac{3}{t}\dot X_t+\nabla f(X_t)=0,
\end{align}
where $X_t=X(t)$, $\dot X$ denotes the first derivative of $X(t)$ with respect to $t$, and  $\Ddot X$ denotes the second derivative of $X(t)$ with respect to $t$.\par
Using Bregman divergence and variational calculus, \cite{WibisonoE7351} found a general ODE 
\begin{align}\label{general-low-res-ODE}
    \Ddot{X}_t+\frac{p+1}{t}\dot X_t+Cp^2t^{p-1}\left[\nabla^2 h(X_t+\frac{t}{p}\dot X_t)\right]^{-1}\nabla f(X_t)=0,
\end{align}
where $p\geq 2$, $C>0$, and $h:\mathcal{X}\rightarrow \mathbb{R}$ is a smooth convex distance generating function with $\mathcal{X}$ denoting the space that $X$ belongs to (e.g. in Euclidean space $h(X)=1/2\|X\|^2$). For the special case of Euclidean space, $p=2$ and $C=1/4$ we recover (\ref{low-res-ode}).\par
To mimic the NAG more accurately, \cite{Shi2021UnderstandingTA} proposed high-resolution ODE
\begin{align}\label{HR_Shi_cvx}
     \Ddot{X}_t + (\frac{3}{t}+\sqrt{s}\nabla^2 f(X_t))\dot{X}_t+(1+\frac{3\sqrt{s}}{2t})\nabla f(X_t)=0.
\end{align}
The \textit{high-resolution} keyword refers to using higher order approximations while analysing the NAG (\ref{eqn_Nest_alg}) in limit of $s\rightarrow 0$. Though the great similarity between (\ref{HR_Shi_cvx}) and the NAG trajectory (see \cite{Shi2021UnderstandingTA} Figure 2), they showed accelerated convergence of a modified version of (\ref{HR_Shi_cvx}) (see \cite{shi2019acceleration}, Appendix C.2). In addition, the correspondence between high-resolution ODEs and the variational perspective is still not clear.\par




\begin{center}
    

% Gradient Info
  
\tikzset {_9z71a3jcd/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
\pgfdeclarehorizontalshading{_6w58ho3wl}{150bp}{rgb(0bp)=(0.72,0.87,0.95);
rgb(40.982142857142854bp)=(0.72,0.87,0.95);
rgb(58.83928571428571bp)=(0.4,0.65,0.95);
rgb(100bp)=(0.4,0.65,0.95)}

% Gradient Info
  
\tikzset {_3w93yvb1b/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
\pgfdeclarehorizontalshading{_fsfb3dlhy}{150bp}{rgb(0bp)=(0.72,0.87,0.95);
rgb(40.982142857142854bp)=(0.72,0.87,0.95);
rgb(58.83928571428571bp)=(0.4,0.65,0.95);
rgb(100bp)=(0.4,0.65,0.95)}

% Gradient Info
  
\tikzset {_b3hmqjp7i/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
\pgfdeclarehorizontalshading{_nccsof0wj}{150bp}{rgb(0bp)=(0.72,0.87,0.95);
rgb(40.982142857142854bp)=(0.72,0.87,0.95);
rgb(58.83928571428571bp)=(0.4,0.65,0.95);
rgb(100bp)=(0.4,0.65,0.95)}

% Gradient Info
  
\tikzset {_eq5sisz4j/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
\pgfdeclarehorizontalshading{_9ppy1ngqz}{150bp}{rgb(0bp)=(0.72,0.87,0.95);
rgb(40.982142857142854bp)=(0.72,0.87,0.95);
rgb(58.83928571428571bp)=(0.4,0.65,0.95);
rgb(100bp)=(0.4,0.65,0.95)}
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,212); %set diagram left start at 0, and has height of 212

%Shape: Ellipse [id:dp6049741184975286] 
\path  [shading=_6w58ho3wl,_9z71a3jcd] (196,101.5) .. controls (196,90.45) and (243.23,81.5) .. (301.5,81.5) .. controls (359.77,81.5) and (407,90.45) .. (407,101.5) .. controls (407,112.55) and (359.77,121.5) .. (301.5,121.5) .. controls (243.23,121.5) and (196,112.55) .. (196,101.5) -- cycle ; % for fading 
 \draw   (196,101.5) .. controls (196,90.45) and (243.23,81.5) .. (301.5,81.5) .. controls (359.77,81.5) and (407,90.45) .. (407,101.5) .. controls (407,112.55) and (359.77,121.5) .. (301.5,121.5) .. controls (243.23,121.5) and (196,112.55) .. (196,101.5) -- cycle ; % for border 

%Straight Lines [id:da627890920362774] 
\draw    (301.5,121.5) -- (301.5,149) ;
\draw [shift={(301.5,151)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4116865783120667] 
\draw    (212,171) -- (187.5,171) ;
\draw [shift={(185.5,171)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da202132937686885] 
\draw    (391,171) -- (416.5,171) ;
\draw [shift={(418.5,171)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Ellipse [id:dp1525321592758131] 
\path  [shading=_fsfb3dlhy,_3w93yvb1b] (97,171) .. controls (97,159.95) and (116.81,151) .. (141.25,151) .. controls (165.69,151) and (185.5,159.95) .. (185.5,171) .. controls (185.5,182.05) and (165.69,191) .. (141.25,191) .. controls (116.81,191) and (97,182.05) .. (97,171) -- cycle ; % for fading 
 \draw   (97,171) .. controls (97,159.95) and (116.81,151) .. (141.25,151) .. controls (165.69,151) and (185.5,159.95) .. (185.5,171) .. controls (185.5,182.05) and (165.69,191) .. (141.25,191) .. controls (116.81,191) and (97,182.05) .. (97,171) -- cycle ; % for border 

%Shape: Ellipse [id:dp687810020824998] 
\path  [shading=_nccsof0wj,_b3hmqjp7i] (212,171) .. controls (212,159.95) and (252.07,151) .. (301.5,151) .. controls (350.93,151) and (391,159.95) .. (391,171) .. controls (391,182.05) and (350.93,191) .. (301.5,191) .. controls (252.07,191) and (212,182.05) .. (212,171) -- cycle ; % for fading 
 \draw   (212,171) .. controls (212,159.95) and (252.07,151) .. (301.5,151) .. controls (350.93,151) and (391,159.95) .. (391,171) .. controls (391,182.05) and (350.93,191) .. (301.5,191) .. controls (252.07,191) and (212,182.05) .. (212,171) -- cycle ; % for border 

%Shape: Ellipse [id:dp7396459776231428] 
\path  [shading=_9ppy1ngqz,_eq5sisz4j] (419,170.83) .. controls (419,159.78) and (438.81,150.83) .. (463.25,150.83) .. controls (487.69,150.83) and (507.5,159.78) .. (507.5,170.83) .. controls (507.5,181.87) and (487.69,190.83) .. (463.25,190.83) .. controls (438.81,190.83) and (419,181.87) .. (419,170.83) -- cycle ; % for fading 
 \draw   (419,170.83) .. controls (419,159.78) and (438.81,150.83) .. (463.25,150.83) .. controls (487.69,150.83) and (507.5,159.78) .. (507.5,170.83) .. controls (507.5,181.87) and (487.69,190.83) .. (463.25,190.83) .. controls (438.81,190.83) and (419,181.87) .. (419,170.83) -- cycle ; % for border 


% Text Node
\draw (219.5,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont Continuous-time perspective}};
% Text Node
\draw (223,162) node [anchor=north west][inner sep=0.75pt]  [font=\large] [align=left] {{\fontfamily{ptm}\selectfont Variational perspective}};
% Text Node
\draw (432,164) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont HR-ODEs}};
% Text Node
\draw (112,163) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont LR-ODEs}};
% Text Node
\draw (394.5,152) node [anchor=north west][inner sep=0.75pt]   [align=left] {?};


\end{tikzpicture}
\end{center}

In this work, first we will show that the high-resolution ODEs can be recovered through variational perspective in Euclidean space. To do so, we leverage the forced Euler-Lagrange equation and treat the higher-order terms as non-conservative forces in our system.
 Second, we will recover the modified version of (\ref{HR_Shi_cvx}) (and other high-resolution ODEs previously found) through variational perspective. At last, we will discretize the modified ODE using Semi-Implicit Euler (SIE) integrator and derive a new presentation of the NAG algorithm. Through a Lyapunov analysis, we will improve the convergence rate for gradient norm minimization from
$$ \min_{0\leq i\leq k}\|\nabla f(x_i)\|^2 \leq \frac{8568}{(k+1)^3s^2}\|x_0-x^*\|^2,$$
 for $0< s\leq 1/(3L)$ and $k\geq0$ \cite{Shi2021UnderstandingTA}, to 
 $$   \min_{0\leq i\leq k-1}\|\nabla f(x_i)\|^2 \leq \frac{12}{k^3s^2}\|x_0-x^*\|^2,$$
for $0< s\leq 1/L$ and $k\geq 1$.\par
The rest of the paper is organized as follows. In Section ...

\section{Related Work}\label{sec_relwork}
Polyak's Heavy-Ball (HB) method was the first momentum-based method which could accelerate relative to (\ref{gradient_descent}) \cite{Polyak1963GradientMF}. The HB method could achieve the optimal $\mathcal{O}(1/k^2)$ rate of convergence. However, this method was shown to be incapable of accelerating every strongly convex function globally \cite{Lessard2016AnalysisAD}. Later, Nesterov modified the HB and introduced (\ref{eqn_Nest_alg}). The NAG method achieved global convergence with a rate of $\mathcal{O}(1/k^2)$ for smooth convex functions \cite{Nesterov1983AMF}. Nesterov used estimate sequence technique to show the convergence of the NAG method. This technique does not provide immediate insights toward the success of the NAG algorithm in acceleration. Thus, many have tried different approaches to understand the essence of acceleration. \par
\cite{JMLR:v17:15-084} proposed a continuous-time perspective on the NAG method. They analysed the method by letting the step-size to zero. The result of their analysis was a second-order ODE. This ODE remains the same if you repeat the analysis for the HB method. Therefore, the continuous-time analysis needed more accurate formulation. \cite{Shi2021UnderstandingTA} did the same analysis with higher accuracy which resulted in high-resolution ODEs. These ODEs were different for the HB and the NAG methods \cite{shi2019acceleration}. Extensions of this work are found in \cite{chen2022gradient,chen2022accelerating}. In \cite{chen2022gradient}, the authors use the phase-space representation of the NAG method and improve the previously found gradient norm minimization convergence rate in \cite{Shi2021UnderstandingTA}. After finishing this work, the authors found that the mentioned rate is almost the same as the rate found in this work. However, here the phase-space representation of the NAG method is not used. \par 
On similar line of work to continuous-time analysis, \cite{WibisonoE7351} introduced a variational perspective on accelerated methods. This led to a general (non-Euclidean) ODE which contained the ODE found by \cite{JMLR:v17:15-084} as a special case. Their work was based on the choice of a Lagrangian and its corresponding parameters. Since the choice of Lagrangian was not unique, \cite{wilson2021lyapunov} provided a variational perspective on different accelerated first-order methods using a second Lagrangian. \cite{7963773} found a family of accelerated dual algorithms for constrained convex minimization problem through similar variational approach. Recently, \cite{zhang2021rethinking} showed that the second-variation also plays an important role in optimality of the ODE found by \cite{JMLR:v17:15-084}. Specifically, they showed that if the time duration is long enough, then the mentioned ODE for the NAG algorithm is the saddle point to the action functional. \par
The dynamical system perspective on the NAG method was studied in \cite{muehlebach2019dynamical}. They showed that the NAG method is recovered from SIE discretization of an ODE. The mentioned ODE was not the result of a vanishing step-size argument. They found that a curvature-dependent damping term accounts for the acceleration phenomenon. Interestingly, \cite{chen2022gradient} also used similar ODE without the SIE discretization. They showed that implicit-velocity is the reason of the acceleration. In a recent analysis, \cite{muehlebach2023accelerated} explores the connections between non-smooth dynamical systems and first-order methods for constrained optimization.
\section{Preliminaries \& Notation}\label{sec_prel}
In this section, necessary definitions and notations are provided. 
Take Lipschitz constant $L>0$. Consider $\mathcal{F}_L$ as the class of $L$-smooth convex functions on $\mathbb{R}^n$. This indicates that for $f\in\mathcal{F}_L$ we have
\begin{align}\label{convexity}
    f(x)\geq f(y)+\langle \nabla f(y),x-y \rangle, \tag{Convexity}
\end{align}
\begin{align}\label{smoothness}
    \|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|.\tag{L-Lipschitz}
\end{align}
because the gradient of $f$ is $L$-Lipschitz smooth for any $x,y\in \mathbb{R}^d$. Here, $\|.\|$ denotes the Euclidean norm. For every $f\in\mathcal{F}_L$ we have
\begin{align}\label{cvx-smthness}
  f(x)-f(y)\leq \langle \nabla f(x),x-y \rangle-\frac{1}{2L}\|x-y\|^2. 
\end{align}
Take $h:\mathcal{X}\rightarrow \mathbb{R}$ as a smooth convex distance generating function with $\mathcal{X}$ denoting the space that $X$ belongs to. Then, the Bregman divergence can be used as a measure of distance in $\mathcal{X}$
\begin{align}\label{BregmanDivergence}
    D_h(y,x)= h(y)-h(x)-\langle \nabla h(x),y-x\rangle.
\end{align}
Note that $D_h(y,x)\geq 0$ due to the convexity of $h$. For the special case of $h(\cdot)=1/2\|\cdot\|^2$, (\ref{BregmanDivergence}) reduces to Euclidean distance. We will denote first-order time derivative with single over-dot $\dot X_t =\tfrac{d}{dt}X_t$ and  second-order time derivative with double over-dot $\Ddot X_t =\tfrac{d^2}{dt^2}X_t$. Gradient and Hessian of function $f:\mathbb R^n\rightarrow \mathbb R$ are denoted with $\nabla f\in \mathbb R^{n}$ and $\nabla ^2 f\in \mathbb R^{n\times n}$ respectively. 
\section{Low-Resolution Accelerated ODE}\label{sec_LR_ODE}
In this section we briefly explain the variational approach that results in (\ref{general-low-res-ODE}) in Euclidean case. Consider the following Lagrangian for convex continuously differentiable function $f : \mathcal{X}\rightarrow \mathbb{R}$ 
\begin{align}\label{Lagrangian1}
    \mathcal{L}(X_t,\dot{X}_t,t) =e^{\alpha_t+\gamma_t}(D_h(X_t,X_t+e^{-\alpha_t}\dot{X}_t)-e^{\beta_t}f(X_t)), 
\end{align}
where $\dot{X}_t\in \mathbb{R}^d$ is the velocity and $\alpha_t,\beta_t,\gamma_t:\mathbb{T}\rightarrow \mathbb{R}$ are continuously differentiable functions of time and correspond to the weighting of velocity, the potential function and the overall damping. In the special case of $\mathcal{X}=\mathbb R^n$ and $h(X_t)=1/2\|X_t\|^2$, the Lagrangian (\ref{Lagrangian1}) reduces to 
\begin{align}\label{Lagrangian2}
    \mathcal{L}(X_t,\dot{X}_t,t) =e^{\alpha_t+\gamma_t}(\frac{1}{2}\|e^{-\alpha_t}\dot{X}_t\|^2-e^{\beta_t}f(X_t)). 
\end{align}
 From the variational calculus, define the action on the curves $\{X_t:t\in \mathbb R\}$ as the functional $\mathcal{A}(X)=\int_{\mathbb R}\mathcal{L}(X_t,\dot X_t,t)dt $. The curve which is a stationary point tothe problem of minimizing the action $\mathcal{A}(X)$, will satisfy the Euler Lagrange equation
\begin{align}\label{EL}
    \frac{d}{dt}\left\{  \frac{\partial \mathcal{L}}{\partial \dot{X}_t}(X_t,\dot{X}_t,t)  \right\}=\frac{\partial \mathcal{L}}{\partial X}(X_t,\dot{X}_t,t),
\end{align}
Substituting (\ref{Lagrangian2}) in (\ref{EL}) and considering the ideal scaling conditions 
\begin{align}\label{ideal_scaling_conditions}
    \dot{\beta}_t\leq e^{\alpha_t} \quad, \dot{\gamma}_t = e^{\alpha_t}
\end{align}
 gives
\begin{align}\label{LRgeneral_form}
    \Ddot{X}_t+(e^{\alpha_t}-\dot{\alpha}_t)\dot{X}_t+e^{2\alpha_t+\beta_t}\nabla f(X_t)=0.
\end{align}
By setting 
\begin{align}
    \alpha_t &= \log p - \log t\nonumber\\
    \beta_t &= p\log t +\log C\nonumber\\
    \gamma_t &= p\log t\nonumber
\end{align}
the continuous accelerated ODE is represented as
\begin{align}\label{LR_cnts_form}
        \Ddot{X}_t+ \frac{p+1}{t}\dot{X}_t+Cp^2t^{p-2}\nabla f(X_t)=0.
\end{align}
This ODE has a convergence rate of $\mathcal{O}(1/t^p)$ \cite{WibisonoE7351}.
\begin{thm}[Theorem 2.1 \cite{WibisonoE7351}]
    Under the ideal scaling conditions (\ref{ideal_scaling_conditions}), the solution to (\ref{LR_cnts_form}) satisfies
    $$f(X_t)-f(x^*)\leq \mathcal{O}(\frac{1}{t^p}),$$
    where $x^*=\arg\min_X f(X)$.
\end{thm}
In the remaining of the paper, ODE (\ref{LR_cnts_form}) is referred to as the low-resolution ODE. Wibisono et al., applied rate-matching discretization methodology on the ODE (\ref{LR_cnts_form}) to derive their accelerated algorithm. Their final method was 
\begin{align}\label{rate_match_1}
    \left\{\begin{array}{l}
    x_{k+1}=\frac{p}{k+p}z_k+\frac{k}{k+p}y_k,\\
    y_{k}=\arg \min_y\left\{ f_{p-1}(y,x_{k})+\frac{N}{p\epsilon}\|y-x_k\|^p  \right\},   \\
        z_{k}=\arg \min_z\left\{ Cpk^{(p-1)}\langle 
\nabla f(y_{k}),z \rangle+\frac{1}{2\epsilon}\|z-z_{k-1}\|^2  \right\},    
    \end{array}\right.
\end{align}
where $k^{(p-1)}:=k(k+1)\ldots (k+p-2)$ and $f_{p-1}(y,x)=\sum_{i=0}^{p-1}\frac{1}{i!}\langle \nabla^i f(x),y-x^i \rangle$ is the $(p-1)$-st order Taylor expansion of $f$ centered around $x$.\par
The variational perspective scheme presented above, gave rise to numerous detailed understandings of other methods like accelerated mirror descent \cite{nesterov2003introductory}, accelerated universal methods \cite{nesterov2015universal}, and even Frank-Wolfe algorithms \cite{frank1956algorithm,wilson2021lyapunov}. Therefore, it is important to provide an analogy for the high-resolution ODEs as well.
\section{High-Resolution Accelerated ODE}\label{sec_HR_ODE}
In this section, high-resolution ODEs are calculated through variational perspective. Consider the high resolution terms (i.e. terms having step-size in their coefficients in e.g. ODE (\ref{HR_Shi_cvx})) as friction (external forces) which are necessary gradient correction terms \cite{Shi2021UnderstandingTA}. To do so, we modify (\ref{EL}) to the forced Euler Lagrange equation
\begin{align}\label{fEL}
        \frac{d}{dt}\left\{  \frac{\partial \mathcal{L}}{\partial \dot{X}_t}(X_t,\dot{X}_t,t)  \right\}-\frac{\partial \mathcal{L}}{\partial X_t}(X_t,\dot{X}_t,t)=F 
\end{align}
which itself is the result of integration by parts of Lagrange d'Alembert principle \cite{campos2021discrete}. In (\ref{fEL}), $F$ is the external force.\par 
Consider the same Lagrangian as (\ref{Lagrangian2}). Then,
\begin{align}\label{Lagrange_par_der}
  &   \frac{\partial \mathcal{L}}{\partial \dot{X}_t}(X_t,\dot{X}_t,t)  =e^{\gamma_t}(e^{-\alpha_t}\dot{X}_t),\nonumber\\
  & \frac{\partial \mathcal{L}}{\partial X_t}(X_t,\dot{X}_t,t)= -e^{\gamma_t+\alpha_t+\beta_t}(\nabla f(X_t)).
\end{align}
Substituting (\ref{Lagrange_par_der}) in (\ref{fEL}) gives
\begin{align}\label{HR_general_ODE_F}
    \Ddot{X}_t + (\dot{\gamma}_t-\dot{\alpha}_t)\dot{X}_t+e^{2\alpha_t+\beta_t}\nabla f(X_t) =e^{\alpha_t-\gamma_t} F
\end{align}
\paragraph{Convex Functions:} Here, we focus on two specific choices of $F$ which correspond to the high-resolution ODEs in \cite{pmlr-v108-laborde20a} and \cite{shi2019acceleration} for convex function.

$\boldsymbol {F = -\sqrt{s}e^{\gamma_t}\frac{d}{dt}[e^{-\alpha_t}\nabla f(X)]}:$ 
    In this case, (\ref{HR_general_ODE_F}) gives
    \begin{align}\label{HR_general_ODE_F_laborde}
    \Ddot{X}_t + (\dot{\gamma}_t-\dot{\alpha}_t)\dot{X}_t+e^{2\alpha_t+\beta_t}\nabla f = -\sqrt{s}e^{\alpha_t}\frac{d}{dt}[e^{-\alpha_t}\nabla f(X_t)].
\end{align}
The convergence of (\ref{HR_general_ODE_F_laborde}) is shown in the following theorem.
\begin{thm}\label{Theorem_ODE_laborde}
Under the ideal scaling conditions $\dot\beta_t\leq e^{\alpha_t}, \dot\gamma_t=e^{\alpha t}$, the dynamic (\ref{HR_general_ODE_F_laborde}) for convex function $f$ will satisfy 
$$f(X_t)-f(x^*)\leq \mathcal{O}(e^{-\beta_t})$$
\end{thm}
\begin{proof}
Consider the Lyapunov function 
\begin{align}\label{lyap_theorem_C}
    \varepsilon(t)=\frac{1}{2}\|X_t+e^{-\alpha_t}\dot X_t-x^*+\sqrt{s}e^{-\alpha_t}\nabla f(X_t)\|^2+e^{\beta_t}(f(X_t)-f(x^*)).
\end{align}
Taking derivative with respect to $t$ gives
\begin{align}\label{prf_thm1_eqn1}
    \frac{d \varepsilon}{dt}=&\langle \frac{d}{dt}(X_t+e^{-\alpha_t}\dot X_t-x^*+\sqrt{s}e^{-\alpha_t}\nabla f(X_t)),X_t+e^{-\alpha_t}\dot X_t-x^*+\sqrt{s}e^{-\alpha_t}\nabla f(X_t)\rangle\nonumber\\
    & +\dot \beta_t e^{\beta_t}(f(X_t)-f(x^*))+e^{\beta_t}\dot X_t\nabla f(X_t).
\end{align}
Note that (\ref{HR_general_ODE_F_laborde}) can be represented as
\begin{align}\label{HR_general_der_format_laborde}
    \frac{d}{dt}\left[X_t+e^{-\alpha_t}\dot X_t+\sqrt{s}e^{-\alpha_t}\nabla f(X_t)\right]=-e^{\alpha_t+\beta_t}\nabla f(X_t).
\end{align}
Using (\ref{HR_general_der_format_laborde}) in (\ref{prf_thm1_eqn1}) we have
\begin{align}
     \frac{d \varepsilon}{dt}=& \langle -e^{\alpha_t+\beta_t}\nabla f(X_t),X_t+e^{-\alpha_t}\dot X_t-x^*+\sqrt{s}e^{-\alpha_t}\nabla f(X_t) \rangle \nonumber \\
     &+\dot \beta_t e^{\beta_t}(f(X_t)-f(x^*))+e^{\beta_t}\dot X_t\nabla f(X_t)\nonumber\\
     &= -e^{\alpha_t+\beta_t}\langle\nabla f(X_t),X_t-x^*\rangle -e^{\beta_t}\langle \nabla f(X_t),\dot X_t\rangle -\sqrt{s}e^{\beta_t}\|\nabla f(X_t)\|^2\nonumber\\
     &+\dot \beta_te^{\beta_t}(f(X_t)-f(x^*))+e^{\beta_t}\langle \dot X_t,\nabla f(X_t) \rangle\nonumber\\
     & \overset{\text{(\ref{convexity}})}{\leq} -e^{\alpha_t+\beta_t}(f(X_t)-f(x^*))+\dot \beta_te^{\beta_t}(f(X_t)-f(x^*))\nonumber\\
     & = -e^{\beta_t}\left[(e^{\alpha_t}-\dot \beta_t)(f(X_t)-f(x^*))\right].\nonumber
\end{align}
Utilizing the ideal scaling condition $\dot \beta_t\leq e^{\alpha_t}$ we have
\begin{align}
     \frac{d \varepsilon}{dt}\leq 0.\nonumber
\end{align}
Thus, for the initialization point $t_0$ we have
$$e^{\beta_t}(f(X_t)-f(x^*))\leq \varepsilon(t)\leq \varepsilon(t_0),$$
and the proof is complete.
\end{proof}
Choosing parameters as
\begin{align}\label{prams_general}
        \alpha_t&=\log (n(t)),\nonumber\\
    \beta_t&=\log (q(t)/n(t)),\\
    \dot\gamma_t&=e^{\alpha_t}=n(t),\nonumber
\end{align}
in (\ref{HR_general_ODE_F_laborde}) gives
\begin{align}\label{HR_cnts_general_npq_laborde}
\left\{
\begin{array}{l}
     \Ddot{X}_t + (n(t)-\frac{\dot n(t)}{n(t)}+\sqrt{s}\nabla^2 f(X_t))\dot{X}_t+(n(t)q(t)-\sqrt{s} \frac{\dot n(t)}{n(t)})\nabla f(X_t)=0,   \\
    F = -\sqrt{s}e^{\gamma_t}\frac{d}{dt}[e^{-\alpha_t}\nabla f(X)],  
\end{array}\right.
\end{align}
which reduces to 
\begin{align}\label{HR_cnts_general_npq2}
     \Ddot{X}_t + (\frac{p+1}{t}+\sqrt{s}\nabla^2 f(X_t))\dot{X}_t+(Cp^2t^{p-2}+\frac{\sqrt{s}}{t})\nabla f(X_t)=0
\end{align}
by taking $n(t)=\frac{p}{t},q(t)=Cpt^{p-1}$. Indeed, this is a more general form of the high-resolution ODE found in \cite{pmlr-v108-laborde20a}.

     $ \boldsymbol{F = -\sqrt{s}e^{\gamma_t-\beta_t}\frac{d}{dt}\left[e^{-(\alpha_t-\beta_t)}\nabla f(X_t)\right]}:$
    In this case, replacing $F$ in (\ref{HR_general_ODE_F}) gives
    \begin{align}\label{HR_general_ODE_F_Shi}
    \Ddot{X}_t + (\dot{\gamma}_t-\dot{\alpha}_t)\dot{X}_t+e^{2\alpha_t+\beta_t}\nabla f = -\sqrt{s}e^{\alpha_t-\beta_t}\frac{d}{dt}[e^{-(\alpha_t-\beta_t)}\nabla f(X_t)],
\end{align}
which has the following convergence result.
\begin{thm}\label{Theorem_ODE_Shi}
Under the modified ideal scaling conditions $\dot\beta_t\leq e^{\alpha_t}, \dot\gamma_t=e^{\alpha t}, \Ddot{\beta}_t\leq e^{\alpha_t}\dot \beta_t + 2\dot \alpha_t \dot \beta_t$, the dynamic (\ref{HR_general_ODE_F_Shi}) for convex function $f$ will satisfy 
$$f(X_t)-f(x^*)\leq \mathcal{O}(\frac{1}{e^{\beta_t}+\sqrt{s}e^{-2\alpha_t}\dot \beta_t})$$
\end{thm}
\begin{proof}
Consider the Lyapunov function 
\begin{align}\label{lyap_theorem3}
    \varepsilon(t)=\frac{1}{2}\|X_t+e^{-\alpha_t}\dot X_t-x^*+\sqrt{s}e^{-\alpha_t}\nabla f(X_t)\|^2+(e^{\beta_t}+\sqrt{s}e^{-2\alpha_t}\dot \beta_t)(f(X_t)-f(x^*)).
\end{align}
Taking derivative with respect to $t$ gives
\begin{align}\label{prf_thm3_eqn1}
    \frac{d \varepsilon}{dt}=&\langle \frac{d}{dt}(X_t+e^{-\alpha_t}\dot X_t-x^*+\sqrt{s}e^{-\alpha_t}\nabla f(X_t)),X_t+e^{-\alpha_t}\dot X_t-x^*+\sqrt{s}e^{-\alpha_t}\nabla f(X_t)\rangle\nonumber\\
    & +(\dot \beta_te^{\beta_t}-\sqrt{s}(2\dot \alpha_t)e^{-2\alpha_t}\dot \beta_t +\sqrt{s}e^{-\alpha_t}\Ddot{\beta_t})(f(X_t)-f(x^*))\nonumber\\
    &+(e^{\beta_t}+\sqrt{s}e^{-2\alpha_t}\dot \beta_t)\dot X_t\nabla f(X_t).
\end{align}
Note that (\ref{HR_general_ODE_F_laborde}) can be represented as
\begin{align}\label{HR_general_der_format_Shi}
    \frac{d}{dt}\left[X_t+e^{-\alpha_t}\dot X_t+\sqrt{s}e^{-\alpha_t}\nabla f(X_t)\right]=-\left(e^{\alpha_t+\beta_t}+\sqrt{s}e^{-\alpha_t}\dot \beta_t \right)\nabla f(X_t).
\end{align}
Using (\ref{HR_general_der_format_Shi}) in (\ref{prf_thm3_eqn1}) we have
\begin{align}
     \frac{d \varepsilon}{dt}=& \langle -\left(e^{\alpha_t+\beta_t}+\sqrt{s}e^{-\alpha_t}\dot \beta_t \right)\nabla f(X_t),X_t+e^{-\alpha_t}\dot X_t-x^*+\sqrt{s}e^{-\alpha_t}\nabla f(X_t) \rangle \nonumber \\
     &+(\dot \beta_te^{\beta_t}-\sqrt{s}(2\dot \alpha_t)e^{-2\alpha_t}\dot \beta_t +\sqrt{s}e^{-2\alpha_t}\Ddot{\beta_t})(f(X_t)-f(x^*))\nonumber\\
    &+(e^{\beta_t}+\sqrt{s}e^{-2\alpha_t}\dot \beta_t)\langle\nabla f(X_t),\dot X_t\rangle,\nonumber\\
     &= -\left(e^{\alpha_t+\beta_t}+\sqrt{s}e^{-\alpha_t}\dot \beta_t \right)\langle\nabla f(X_t),X_t-x^*\rangle -\left(e^{\beta_t}+\sqrt{s}e^{-2\alpha_t}\dot \beta_t \right)\langle \nabla f(X_t),\dot X_t\rangle \nonumber\\
     &-\sqrt{s}\left(e^{\beta_t}+\sqrt{s}e^{-2\alpha_t}\dot \beta_t \right)\|\nabla f(X_t)\|^2\nonumber\\
     &+(\dot \beta_te^{\beta_t}-\sqrt{s}(2\dot \alpha_t)e^{-2\alpha_t}\dot \beta_t +\sqrt{s}e^{-2\alpha_t}\Ddot{\beta_t})(f(X_t)-f(x^*))\nonumber\\
    &+(e^{\beta_t}+\sqrt{s}e^{-2\alpha_t}\dot \beta_t)\langle\nabla f(X_t),\dot X_t\rangle,\nonumber\\
     & \overset{\text{(\ref{convexity}})}{\leq} -\left(e^{\alpha_t+\beta_t}+\sqrt{s}e^{-\alpha_t}\dot \beta_t \right)(f(X_t)-f(x^*))\nonumber\\
     &+(\dot \beta_te^{\beta_t}-\sqrt{s}(2\dot \alpha_t)e^{-2\alpha_t}\dot \beta_t +\sqrt{s}e^{-2\alpha_t}\Ddot{\beta_t})(f(X_t)-f(x^*)),\nonumber\\
     & = -\left[e^{\beta_t}(e^{\alpha_t}-\dot \beta_t)+\sqrt{s}e^{-\alpha_t}(\dot \beta_t+2\dot \alpha_t e^{-\alpha_t}\dot \beta_t-e^{-\alpha_t}\Ddot{\beta}_t)\right](f(X_t)-f(x^*)).\nonumber
\end{align}
Utilizing the modified ideal scaling conditions $\dot \beta_t\leq e^{\alpha_t}$ and $\Ddot{\beta}_t\leq e^{\alpha_t}\dot \beta_t + 2\dot \alpha_t \dot \beta_t$ we have
\begin{align}
     \frac{d \varepsilon}{dt}\leq 0.\nonumber
\end{align}
Thus, for the initialization point $t_0$ we have
$$(e^{\beta_t}+\sqrt{s}e^{-2\alpha_t}\dot \beta_t)(f(X_t)-f(x^*))\leq \varepsilon(t)\leq \varepsilon(t_0),$$
and the proof is complete.
\end{proof}
As it can be inferred, Theorem \ref{Theorem_ODE_Shi} reveals faster convergence rate than Theorem \ref{Theorem_ODE_laborde} which suggests the superiority of the high-resolution ODE found by \cite{Shi2021UnderstandingTA} to the ODE from \cite{pmlr-v108-laborde20a}. Taking the same parameters (\ref{prams_general}) gives
 \begin{align}\label{HR_cnts_general_npq_SHI}
\left\{
\begin{array}{l}
     \Ddot{X}_t + (n(t)-\frac{\dot n(t)}{n(t)}+\sqrt{s}\nabla^2 f(X_t))\dot{X}_t+(n(t)q(t)-\sqrt{s} (\frac{\dot n(t)}{n(t)}-\frac{\dot q(t)n(t)-\dot n(t)q(t)}{n(t)q(t)}))\nabla f(X_t)=0,   \\
     F = -\sqrt{s}e^{\gamma_t-\beta_t}\frac{d}{dt}\left[e^{-(\alpha_t-\beta_t)}\nabla f(X_t)\right]. 
\end{array}\right.
\end{align}
which reduces to
\begin{align}\label{HR_cnts_general_npq_p}
     \Ddot{X}_t + (\frac{p+1}{t}+\sqrt{s}\nabla^2 f(X_t))\dot{X}_t+(Cp^2t^{p-2}+\frac{\sqrt{s}(p+1)}{t})\nabla f(X_t)=0
\end{align}
for $n(t)=\frac{p}{t},q(t)=Cpt^{p-1}$. Note that setting $C=1/4,p=2$ will lead to the ODE 
\begin{align}\label{HR_cnts_general_stable_ODE}
     \Ddot{X}_t + (\frac{3}{t}+\sqrt{s}\nabla^2 f(X_t))\dot{X}_t+(1+\frac{3\sqrt{s}}{t})\nabla f(X_t)=0
\end{align}
which was shown to be accelerated after SIE discretization in \cite{shi2019acceleration}.
\paragraph{Strongly Convex Functions:} Our analysis is applicable to strongly convex functions as well. Using the Lagrangian proposed in \cite{wilson2021lyapunov} for strongly convex functions we have
\begin{align}\label{strongly_cvx_lagrange}
    \mathcal{L}(X_t,\dot X_t,t)=e^{\alpha_t+\beta_t+\gamma_t}(\mu \frac{1}{2}\|e^{-\alpha_t}\dot X_t\|^2-f(X_t)).
\end{align}
Then the forced Euler-Lagrange equation (\ref{fEL}) becomes
\begin{align}\label{HR_FEL}
    \Ddot{X}+(-\dot \alpha_t + \dot 
 \gamma_t+ \dot \beta_t)\dot X+\frac{1}{\mu}e^{2\alpha_t}\nabla f(X)=\frac{F}{\mu e^{-\alpha_t + \gamma_t + \beta_t}}.
\end{align}
Using $F=-\sqrt{s}e^{\alpha_t+\gamma_t}\frac{d}{dt}(e^{\beta_t}\nabla f(X_t))$ in (\ref{HR_FEL})
we get
\begin{align}\label{sc_eqn1}
    \Ddot{X}+(-\dot \alpha_t + \dot 
 \gamma_t+ \dot \beta_t)\dot X+\frac{1}{\mu}e^{2\alpha_t}\nabla f(X)=\frac{-\sqrt{s}e^{2\alpha_t-\beta_t}\frac{d}{dt}(e^{\beta_t}\nabla f(X_t))}{\mu }.
\end{align}
The following theorem shows the convergence of (\ref{sc_eqn1}).
\begin{thm}\label{Theorem3_1}
    Under the modified ideal scaling conditions $\alpha_t=\alpha$, $\dot \beta_t\leq e^{\alpha_t}$, $\dot \gamma_t=e^{\alpha_t}$, and $\dot \beta_t\geq 0$ the dynamic (\ref{sc_eqn1}) will satisfy
    \begin{align}\label{Theorem32_eqn1}
      f(X_t)-f(x^*)\leq \mathcal{O}(e^{-\beta_t})  
    \end{align}
    for $\mu$-strongly convex function $f$.
\end{thm}
\begin{proof}
    Consider the Lyapunov function
    \begin{align}\label{thm32_eqn_1}
        \varepsilon(t) = e^{\beta_t}\left(\frac{\mu}{2}\|X_t-x^*+e^{-\alpha_t}\dot X_t+\frac{\sqrt{s}e^{\alpha_t}}{\mu}\nabla f(X_t)\|^2+f(X_t)-f(x^*)\right).
    \end{align}
    Taking derivative w.r.t. time gives
    \begin{align}\label{thm32_eqn_2}
       \frac{d\varepsilon(t)}{dt}&=\dot \beta e^{\beta_t}\left(\frac{\mu}{2}\|X_t-x^*+e^{-\alpha_t}\dot X_t+\frac{\sqrt{s}e^{\alpha_t}}{\mu}\nabla f(X_t)\|^2+f(X_t)-f(x^*)\right)\nonumber\\
       & + \mu e^{\beta_t}\left\langle \dot X_t-\dot \alpha_t e^{-\alpha_t}\dot X_t + e^{\alpha_t}\Ddot{X}_t +\frac{\sqrt{s}}{\mu}\dot \alpha_te^{\alpha_t}\nabla f(X_t)+\frac{\sqrt{s}}{\mu}e^{\alpha_t}\nabla ^2f(X_t)\dot X_t\right.\nonumber\\
       &\left. ,X_t-x^*+e^{-\alpha_t}\dot X_t+\frac{\sqrt{s}e^{\alpha_t}}{\mu}\nabla f(X_t)\right\rangle+e^{\beta_t}\langle \nabla f(X_t),\dot X_t \rangle.
    \end{align}
    Next, we will use (\ref{sc_eqn1}) in (\ref{thm32_eqn_2})
    \begin{align}\label{thm32_eqn_3}
        \frac{d\varepsilon(t)}{dt}&=\dot \beta e^{\beta_t}\left(\frac{\mu}{2}\|X_t-x^*+e^{-\alpha_t}\dot X_t+\frac{\sqrt{s}e^{\alpha_t}}{\mu}\nabla f(X_t)\|^2+f(X_t)-f(x^*)\right)\nonumber\\
       & + \mu e^{\beta_t}\left\langle \dot X_t-e^{-\alpha_t}(\dot \gamma_t+\dot \beta_t)\dot X_t  +\frac{e^{\alpha_t }}{\mu}(\sqrt{s}(\dot \alpha_t-\dot \beta_t)-1)\nabla f(X_t)\right.\nonumber\\
       &\left. ,X-x^*+e^{-\alpha_t}\dot X_t+\frac{\sqrt{s}e^{\alpha_t}}{\mu}\nabla f(X_t)\right\rangle+e^{\beta_t}\langle \nabla f(X_t),\dot X_t \rangle,\nonumber\\
   &= \dot \beta_t e^{\beta_t}\left(\frac{\mu}{2}\left[\|X_t-x^*\|^2+e^{-2\alpha_t}\|\dot X_t\|^2+\|\frac{\sqrt{s}e^{\alpha_t}}{\mu}\nabla f(X_t)\|^2+2e^{-\alpha_t}\langle X_t-x^*,\dot X_t\rangle\right.\right.\nonumber\\
   &\left.\left.+\frac{2\sqrt{s}e^{\alpha_t}}{\mu}\langle X_t-x^*,\nabla f(X_t) \rangle+ \frac{2\sqrt{s}}{\mu}\langle \nabla f(X_t),\dot X_t \rangle\right]\right)+\dot \beta_t e^{\beta_t} (f(X_t)-f(x^*) )\nonumber\\
   & +\mu e^{\beta_t}\left[ (1-e^{-\alpha_t}(\dot \gamma_t+\dot \beta_t))\langle \dot X_t,X_t-x^*\rangle+ e^{-\alpha_t}(1-e^{-\alpha_t}(\dot \gamma_t+\dot \beta_t))\|\dot X_t\|^2 \right.\nonumber\\
   & +\frac{\sqrt{s}e^{\alpha_t}}{\mu}(1-e^{-\alpha_t}(\dot \gamma_t+\dot \beta_t))\langle \dot X_t,\nabla f(X_t) \rangle + \frac{e^{\alpha_t }}{\mu}(\sqrt{s}(\dot \alpha_t-\dot \beta_t)-1)\langle \nabla f(X_t) ,X_t-x^*\rangle\nonumber\\
   & \left.+ \frac{(\sqrt{s}(\dot \alpha_t-\dot \beta_t)-1)}{\mu}\langle \nabla f(X_t),\dot X_t \rangle+\frac{\sqrt{s}e^{2\alpha_t}}{\mu^2}(\sqrt{s}(\dot \alpha_t-\dot \beta_t)-1)\|\nabla f(X_t)\|^2\right]\nonumber\\
   & + e^{\beta_t}\langle \nabla f(X_t),\dot X_t \rangle.
    \end{align}
    Now, using strong convexity of $f$ and applying $\dot \alpha_t=\alpha$, $\dot \beta\geq 0$, $\dot \gamma_t=e^{\alpha_t}$, and $\dot \beta_t\leq e^{\alpha_t}$ gives
    \begin{align}\label{thm32_eqn_4}
        \frac{d\varepsilon(t)}{dt}&\leq-\dot \beta_t e^{\beta_t}\|\sqrt{\frac{\mu}{2}}e^{-\alpha_t}\dot X_t\|^2-\sqrt{s}e^{\beta_t}\dot \beta_t\langle \dot X_t,\nabla f(X_t) \rangle-\dot \beta_t e^{\beta_t}\|\frac{\sqrt{s}e^{\alpha_t}}{\sqrt{2\mu}}\nabla f(X_t)\|^2\nonumber\\
        &=-\dot \beta_t e^{\beta_t} \|    \sqrt{\frac{\mu}{2}}e^{-\alpha_t}\dot X_t+  \frac{\sqrt{s}e^{\alpha_t}}{\sqrt{2\mu}}\nabla f(X_t)\|^2\leq 0,
    \end{align}
    and therefore, 
    $$e^{\beta_t}(f(X_t)-f(x^*))\leq \varepsilon(t)\leq \varepsilon(0)$$
    and the proof is complete.
\end{proof}
Taking $\alpha=\log(\sqrt{\mu})$ and $\gamma_t=\beta_t=\sqrt{\mu}t$ in (\ref{sc_eqn1}) gives the high-resolution ODE 
\begin{align}\label{high-res-ODE}
    \Ddot{X}_t+(2\sqrt{\mu}+\sqrt{s}\nabla^2 f(X_t))\dot X_t +(1+\sqrt{\mu s })\nabla f(X_t)=0,
\end{align}
for strongly-convex function $f$ \cite{Shi2021UnderstandingTA}.
\section{Discretization of High-Resolution ODEs}
In this section we will propose a new algorithm which stems from SIE discretization of (\ref{HR_cnts_general_npq_p}). Before discretization, note that \cite{shi2019acceleration} introduces 
\begin{align}\label{HR_Shi_found}
     \Ddot{X}_t + (\frac{3}{t}+\sqrt{s}\nabla^2 f(X_t))\dot{X}_t+(1+\frac{\sqrt{s}(3)}{2t})\nabla f(X_t)=0,
\end{align}
as the high-resolution ODE for the NAG in convex case, but proved convergence for discretization of \begin{align}\label{HR_Shi_discretizes}
     \Ddot{X}_t + (\frac{3}{t}+\sqrt{s}\nabla^2 f(X_t))\dot{X}_t+(1+\frac{\sqrt{s}(3)}{t})\nabla f(X_t)=0.
\end{align}
Interestingly, setting $p=2$ in (\ref{HR_cnts_general_npq_p}) leads to the same ODE (\ref{HR_Shi_discretizes}) which can be represented as
\begin{align}\label{Two_oneline_ODE_perturbed}
   \left\{ \begin{array}{ll}
    & \dot{x}   =     n(t)(v-x)-\sqrt{s}\nabla f(x)\\
     &\dot{v}    =  -q(t)\nabla f(x) - \sqrt{s}\frac{\dot q(t)n(t)-\dot n(t)q(t)}{n^2(t)q(t)} \nabla f(x)
    \end{array}\right.
\end{align}
Applying SIE on (\ref{Two_oneline_ODE_perturbed}) for $n(t_k)=p/t_k,q(t_k)=Cpt_k^{p-1}$ and $p=2,t_k=k\sqrt{s},C=\tfrac{1}{4}$ gives 
\begin{align}\label{new_algorithm}
   \left\{ \begin{array}{ll}
    &x_{k+1}   =    x_{k} + \frac{2}{k}(v_k-x_{k+1})-{s}\nabla f(x_k)\\
     &v_{k+1}    = v_k -\tfrac{1}{2}((k)s)\nabla f(x_{k+1})-s\nabla f(x_{k+1}) 
    \end{array}\right.
\end{align}
 which is exactly the NAG algorithm. This form of derivation is new since Shi et al., did not recover (\ref{HR_Shi_discretizes}). In fact, they showed that discretizing (\ref{HR_Shi_found}) with any of the three integrators (SIE, EE, and IE) cannot guarantee convergence \cite{shi2019acceleration}. \par
Next, we will prove a better rate of convergence for gradient norm minimization of the NAG algorithm than the state of the art. 
\begin{thm}\label{theorem4}
    Consider the update (\ref{Two_oneline_ODE_perturbed}) with $n(t_k)=p/t_k,q(t_k)=Cpt_k^{p-1}$ and $p=2,t_k=k\sqrt{s},C=\tfrac{1}{4}$. Then, if $f$ is convex and $L$-smooth we have
    $$\min_{0\leq i\leq k-1}\|\nabla f(x_i)\|^2 \leq \frac{12}{k^3s^2}\|x_0-x^*\|^2,$$
    and
    $$f(x_k)-f(x^*)\leq \frac{2}{sk(k+2)}\|x_0-x^*\|^2$$
    for $0\leq s\leq 1/L$ and $x_0=v_0$.
\end{thm}
\begin{proof}
    Take the Lyapunov function 
\begin{align}\label{Lyapunov_function}
    \varepsilon(k) = \frac{s(k+2)k}{4}(f(x_k)-f(x^*))+\frac{1}{2}\|x_{k+1}-x^*+\frac{k}{2}(x_{k+1}-x_k)+\frac{ks}{2}\nabla f(x_k)\|^2.
\end{align}
The choice of Lyapunov function is the same as \cite{Shi2021UnderstandingTA}. Note that the second term is equivalent to $\frac{1}{2}\|v_k-x^*\|^2$ through the first line of the update rule (\ref{new_algorithm}). Next, we will show that 
\begin{align}\label{Lyap_1}
    \varepsilon(k+1)-\varepsilon(k)\leq -\frac{s^2k(k+2)}{8}\|\nabla f(x_k)\|^2.
\end{align}
Using (\ref{Lyapunov_function}) we have
\begin{align}\label{Lyap_2}
    \varepsilon(k+1)-\varepsilon(k)&=\frac{s(k+3)(k+1)}{4}(f(x_{k+1})-f(x^*))+\frac{1}{2}\|v_{k+1}-x^*\|^2\nonumber\\
    & - \frac{s(k+2)(k)}{4}(f(x_{k})-f(x^*))+\frac{1}{2}\|v_{k}-x^*\|^2\nonumber\\
    &=\frac{s(k+2)k}{4}(f(x_{k+1})-f(x_k))+\frac{s(2k+3)}{4}(f(x_{k+1})-f(x^*))\nonumber\\
    &+\frac{1}{2}(2\langle v_{k+1}-v_k,v_k-x^* \rangle+\|v_{k+1}-v_k\|^2)\nonumber\\
    &= \frac{s(k+2)k}{4}(f(x_{k+1})-f(x_k))+\frac{s(2k+3)}{4}(f(x_{k+1})-f(x^*))\nonumber\\
    &+\frac{1}{2}(2\langle -s(\frac{k+2}{2})\nabla f(x_{k+1}),x_{k+1}-x^*+\frac{k}{2}(x_{k+1}-x_k)+\frac{ks}{2}\nabla f(x_k) \rangle \nonumber\\
    &+\|s(\frac{k+2}{2})\nabla f(x_{k+1})\|^2)\nonumber\\
    &= \frac{s(k+2)k}{4}(f(x_{k+1})-f(x_k))+\frac{s(2k+3)}{4}(f(x_{k+1})-f(x^*))\nonumber\\
    &-s(\frac{k+2}{2})\langle \nabla f(x_{k+1}),x_{k+1}-x^*\rangle-s\frac{k(k+2)}{4}\langle \nabla f(x_{k+1}),x_{k+1}-x_k\rangle\nonumber\\
    & -s^2(\frac{k(k+2)}{4})\langle \nabla f(x_{k+1}),\nabla f(x_k)\rangle+ \frac{(s(k+2))^2}{8}\|\nabla f(x_{k+1})\|^2
\end{align}
Now, from convexity and smoothness of the function $f$ we have
\begin{align}\label{smooth_convex}
    f(x_{k+1})-f(x_k)\leq \langle \nabla f(x_{k+1}),x_{k+1}-x_k\rangle -\frac{1}{2L}\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2.
\end{align}
Applying (\ref{smooth_convex}) in (\ref{Lyap_2}) we get
\begin{align}\label{Lyap_3}
     \varepsilon(k+1)-\varepsilon(k)&\leq \frac{s(k+2)k}{4}\left[ \langle \nabla f(x_{k+1}),x_{k+1}-x_k\rangle-\frac{1}{2L}\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2 \right]\nonumber\\
     & +\frac{s(2k+3)}{4} \left[ \langle \nabla f(x_{k+1}),x_{k+1}-x^*\rangle-\frac{1}{2L}\|\nabla f(x_{k+1})\|^2 \right]\nonumber\\
     & -s(\frac{k(k+2)}{4})\langle \nabla f(x_{k+1}),x_{k+1}-x_k \rangle-s(\frac{k+2}{2})\langle \nabla f(x_{k+1}),x_{k+1}-x^* \rangle\nonumber\\
     & -s^2(\frac{k(k+2)}{4})\langle \nabla f(x_{k+1}),\nabla f(x_k)\rangle+ \frac{(s(k+2))^2}{8}\|\nabla f(x_{k+1})\|^2\nonumber\\
     &\leq -\frac{s(k+2)k}{8L}\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2-\frac{s(2k+4)}{8L}\|\nabla f(x_{k+1})\|^2 \nonumber\\
     & -2s^2(\frac{k(k+2)}{8})\langle \nabla f(x_{k+1}),\nabla f(x_k)\rangle+\frac{s^2k(k+2)}{8}\|\nabla f(x_{k+1})\|^2\nonumber\\
     &+\frac{s^2(k+2)}{4}\|\nabla f(x_{k+1})\|^2\nonumber\\
     &=  -\frac{s(k+2)k}{8}(\frac{1}{L}-s)\|\nabla f(x_{k+1})\|^2-\frac{s(2k+4)}{8L}\|\nabla f(x_{k+1})\|^2\nonumber\\
     &+2s(\frac{k(k+2)}{8})(\frac{1}{L}-s)\langle \nabla f(x_{k+1}),\nabla f(x_k)\rangle\nonumber\\
     &-\frac{s(k+2)k}{8}(\frac{1}{L}-s)\|\nabla f(x_{k})\|^2-s^2(\frac{k(k+2)}{8})\|\nabla f(x_{k})\|^2\nonumber\\
     &= -\frac{s(k+2)k}{8}(\frac{1}{L}-s)\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2-\frac{s(2k+4)}{8L}\|\nabla f(x_{k+1})\|^2\nonumber\\
     &-s^2(\frac{k(k+2)}{8})\|\nabla f(x_{k})\|^2\leq -s^2(\frac{k(k+2)}{8})\|\nabla f(x_{k})\|^2,
\end{align}
where in the second inequality we used $-\langle \nabla f(x_{k+1}),x_{k+1}-x^* \rangle\leq -\frac{1}{2L}\|\nabla f(x_{k+1})\|^2$ and the last inequality holds as long as $s\leq 1/L$.\par
With (\ref{Lyap_1}) at hand, we can make sum both sides from $i=0$ till $i=k-1$ and get
\begin{align}\label{Lyap_4}
        \varepsilon(k)-\varepsilon(0)&\leq -\frac{s^2}{8}\sum_{i=0}^{k} i(i+2)\|\nabla f(x_i)\|^2\nonumber\\
        & \leq -\frac{s^2}{8}\min_{0\leq i\leq k}\|\nabla f(x_i)\|^2\sum_{i=0}^{k} i(i+2)\nonumber\\
        &=-\frac{s^2}{8}\min_{0\leq i\leq k}\|\nabla f(x_i)\|^2\sum_{i=1}^{k} i(i+2)\nonumber\\
        &=-\frac{s^2}{8}\min_{0\leq i\leq k}\|\nabla f(x_i)\|^2\left[ \frac{k(k+1)(2k+1)}{6}+k(k+1) \right]\nonumber\\
        &\leq -\frac{s^2}{8}\min_{0\leq i\leq k}\|\nabla f(x_i)\|^2\left[ \frac{k(k+1)(2k+1)}{6}\right]\nonumber\\
        &\leq -\frac{k^3s^2}{24}\min_{0\leq i\leq k}\|\nabla f(x_i)\|^2.
\end{align}
Note that $\varepsilon(k)\geq 0$. Therefore, we have
\begin{align}\label{Lyap_5}
   & -\varepsilon(0) \leq -\frac{k^3s^2}{24}\min_{0\leq i\leq k}\|\nabla f(x_i)\|^2\nonumber\\
    \rightarrow & \varepsilon(0)\geq \frac{k^3s^2}{24}\min_{0\leq i\leq k}\|\nabla f(x_i)\|^2.
\end{align}
Next, not ethat for $k=0$, Lyapunov function (\ref{Lyapunov_function}) is equivalent to $1/2\|v_0-x^*\|^2$. With initialization $v_0=x_0$ we get
\begin{align}\label{Lyap_6}
    \frac{1}{2}\|x_0-x^*\|^2&\geq \frac{k^3s^2}{24}\min_{0\leq i\leq k}\|\nabla f(x_i)\|^2,
\end{align}
and therefore,
\begin{align}\label{Lyap_7}
    \min_{0\leq i\leq k}\|\nabla f(x_i)\|^2 \leq \frac{12}{k^3s^2}\|x_0-x^*\|^2,
\end{align}
for $0< s\leq 1/L$ and $k\geq 1$. This rate is improved compared to the previous rate found by \cite{Shi2021UnderstandingTA} which was 
$$ \min_{0\leq i\leq k}\|\nabla f(x_i)\|^2 \leq \frac{8568}{(k+1)^3s^2}\|x_0-x^*\|^2,$$
for $0< s\leq 1/(3L)$ and $k\geq0$.
Also, from (\ref{Lyap_4}) we have $\varepsilon(k)\leq \varepsilon(0) = 1/2\|v_0-x^*\|^2$. Thus,
\begin{align}\label{Lyap_8}
    f(x_k)-f(x^*)\leq \frac{2}{sk(k+2)}\|x_0-x^*\|^2,
\end{align}
since $x_0=v_0$. This completes the proof.
\end{proof}



\subsection{Rate-Matching Implicitly Perturbs}
In this section, we show that the rate-matching technique used in \cite{WibisonoE7351} has an implicit perturbation. We show this by approving that the rate-matching when applied on the low-resolution ODE (\ref{LR_cnts_form}), has (approximately) the same high-resolution ODE as (\ref{HR_cnts_general_npq2}).  \par
The general low-resolution ODE proposed by \cite{WibisonoE7351} is
\begin{align}\label{gen_LR_ODE}
    \Ddot{X}_t+\frac{p+1}{t}\dot X_t +Cp^2t^{p-2}\left[\nabla^2 h(X_t+\frac{t}{p}\dot X_t)\right]^{-1}\nabla f(X_t)=0,
\end{align}
where $C>0$ is a constant. Here, take $h=\frac{1}{2}\|\cdot\|^2$ and thus, $\nabla^2 h(x)=I_d$ with $I_d$ denoting the identity matrix. Wibisono et al., applied rate-matching discretization methodology on the ODE (\ref{gen_LR_ODE}) to derive their accelerated algorithm. Their final method was 
\begin{align}\label{rate_match_1}
    \left\{\begin{array}{l}
    x_{k+1}=\frac{p}{k+p}z_k+\frac{k}{k+p}y_k,\\
    y_{k}=\arg \min_y\left\{ f_{p-1}(y,x_{k})+\frac{N}{p\epsilon}\|y-x_k\|^p  \right\},   \\
        z_{k}=\arg \min_z\left\{ Cpk^{(p-1)}\langle 
\nabla f(y_{k}),z \rangle+\frac{1}{2\epsilon}\|z-z_{k-1}\|^2  \right\},    
    \end{array}\right.
\end{align}
which had a convergence rate of $\mathcal{O}(1/\epsilon k^p)$ and for $p=2$ it reduces to
\begin{align}\label{rate_match_2}
    \left\{\begin{array}{l}
    x_{k+1}=\frac{2}{k+2}z_k+\frac{k}{k+2}y_k,\\
    y_{k}=x_k-\frac{\epsilon}{N}\nabla f(x_k),   \\
    z_{k}=z_{k-1} -2C\epsilon k\nabla f(y_k)  .    
    \end{array}\right.
\end{align}
The update (\ref{rate_match_2}) is not the same as the NAG method, but it has the same order of convergence. On top of that, it was derived from the low-resolution ODE (\ref{gen_LR_ODE}) and its convergence proof is based on a generalization of the estimate sequence technique \cite{baes2009estimate}. It remains an open question to understand the connection between (\ref{rate_match_2}) (which is the result of the rate-matching technique) and the NAG method. \par
To answer the proposed question, we investigate the behaviour of (\ref{rate_match_2}) in limit of $\epsilon \rightarrow 0$.
\begin{prop}
The continuous-time behaviour of (\ref{rate_match_2}) with $C=1/4,N=1$ is
\begin{align}\label{cont_rate_match_2}
    \Ddot{X}_t+(\frac{3}{t}+\sqrt{\epsilon}\nabla f(X_t))\dot X_t+\frac{\sqrt{\epsilon}}{t}\nabla f(X_t)=-\nabla f(Y_t).
\end{align}
If in addition, one approximates $Y_t\approx X_t$, then the high-resolution ODE (\ref{HR_cnts_general_npq2}) is achieved.
\end{prop}
\begin{proof}
    From the update rule (\ref{rate_match_2}) we have
    \begin{align}\label{prop1_eqn1}
        z_k = x_{k+1}+\frac{k}{2}(x_{k+1}-y_k)=x_{k+1}+\frac{k}{2}(x_{k+1}-x_k+\epsilon \nabla f(x_k)).
    \end{align}
    Replacing (\ref{prop1_eqn1}) in the update rule of $z_k$ in (\ref{rate_match_2}), we get
    \begin{align}\label{prop1_eqn2}
        x_{k+1}-x_k+\frac{k}{2}(x_{k+1}-x_k+\epsilon\nabla f(x_k))-\frac{k-1}{2}(x_k-x_{k-1}+\epsilon \nabla f(x_{k-1}))=-\frac{sk}{2}\nabla f(y_k).
    \end{align}
    By rearranging we have
      \begin{align}\label{prop1_eqn3}
        x_{k+1}-x_k&+\frac{1}{2}(x_{k}-x_{k-1})+\frac{\epsilon}{2}\nabla f(x_{k-1})+\frac{k}{2}(x_{k+1}+x_{k+1}-2x_k)\nonumber\\
        &+\frac{k\epsilon}{2}(\nabla f(x_{k}- \nabla f(x_{k-1}))=-\frac{\epsilon k}{2}\nabla f(y_k),\nonumber\\
        \rightarrow \color{blue}\frac{1}{k\sqrt{\epsilon}}(\frac{x_{k+1}-x_k}{\sqrt{\epsilon}})\color{black}&+\color{red}\frac{1}{k\sqrt{\epsilon}}(\frac{x_{k+1}-x_k}{\sqrt{\epsilon}})\color{black}+\color{red}\frac{1}{k\sqrt{\epsilon}}(\frac{x_k-x_{k-1}}{\sqrt{\epsilon}})\color{black} +\frac{1}{k}\nabla f(x_{k-1}) \nonumber\\
        & +\frac{x_{k+2}-2x_k+x_{k-1}}{\epsilon}+\nabla f(x_k)-\nabla f(x_{k-1})=-\nabla f(y_k).
    \end{align}
    Using approximations
    \begin{align}\label{prop1_eqn4}
        \color{blue}\frac{1}{k\sqrt{\epsilon}}(\frac{x_{k+1}-x_k}{\sqrt{\epsilon}})&\approx \frac{\dot X(t_k)}{t_k},\\
        \color{red}\frac{1}{k\sqrt{\epsilon}}(\frac{x_{k+1}-x_k}{\sqrt{\epsilon}})\color{black}+\color{red}\frac{1}{k\sqrt{\epsilon}}(\frac{x_k-x_{k-1}}{\sqrt{\epsilon}})&\approx \frac{1}{t_k}(\dot X(t_k)+\frac{\sqrt{\epsilon}}{2}\Ddot{X}(t_k)+ \dot X(t_k)-\frac{\sqrt{\epsilon}}{2}\Ddot{X}(t_k))\nonumber\\
        &=\frac{1}{t_k}(2 \dot X(t_k))\\
        \frac{x_{k+2}-2x_k+x_{k-1}}{\epsilon}&\approx \Ddot{X}(t_k)\\
        \nabla f(x_k)-\nabla f(x_{k-1})&\approx \sqrt{\epsilon}\nabla^2 f(X(t_k))\dot X(t_k)\\
        X(t)\approx X(t_k)\qquad \dot X(t)&\approx\dot X(t_k)\qquad \Ddot X(t)\approx\Ddot X(t_k) \qquad Y(t_k)\approx Y(t)
    \end{align}
    and $t_k=k\sqrt{\epsilon}$ in (\ref{prop1_eqn3}), we get
\begin{align}\label{prop1_eqn5}
    \Ddot{X}(t)+(\frac{3}{t}+\sqrt{\epsilon}\nabla f(X(t)))\dot X(t)+\frac{\sqrt{\epsilon}}{t}\nabla f(X(y))=-\nabla f(Y(t)).
\end{align}
This, concludes the proof.
\end{proof}
According to \cite{WibisonoE7351}, the distance between sequences $x_k$ and $y_k$ in (\ref{rate_match_2}) decreases with rate of order $\sqrt{\epsilon}$. Therefore, if the approximation $X_t\approx Y_t$ is made (due to $\epsilon\rightarrow 0$), (\ref{cont_rate_match_2}) will be equivalent to
\begin{align}\label{cont_rate_match_2_perturb}
    \left\{\begin{array}{l}
         \dot X_t = \frac{2}{t}(Z_t-X_t)-\sqrt{\epsilon}\nabla f(X_t),  \\
          \dot Z_t = -\frac{t}{2}\nabla f(X_t).
    \end{array}
    \right.
\end{align}
which is the perturbation of the low-resolution ODE
\begin{align}\label{cont_rate_match_2_perturb2}
    \left\{\begin{array}{l}
         \dot X_t = \frac{2}{t}(Z_t-X_t),  \\
          \dot Z_t = -\frac{t}{2}\nabla f(X_t).
    \end{array}
    \right.
\end{align}
Note that (\ref{cont_rate_match_2_perturb2}) is equivalent to (\ref{LR_cnts_form}) for $p=2$. In this sense, rate-matching implicitly perturbs the low-resolution ODE. The question that naturally arises is that when do we recover the high-resolution ODE (\ref{HR_cnts_general_stable_ODE}) from rate-matching technique? To show this, we will first perturb the low-resolution ODE (\ref{cont_rate_match_2_perturb2}) in the second line. Then, the rate-matching discretization is applied. Perturbing (\ref{cont_rate_match_2_perturb2}) gives
\begin{align}\label{cont_rate_match_2_perturb3}
    \left\{\begin{array}{l}
         \dot X_t = \frac{2}{t}(Z_t-X_t),  \\
          \dot Z_t = -\frac{t}{2}\nabla f(X_t)-\sqrt{\epsilon}\nabla f(X_t).
    \end{array}
    \right.
\end{align}
Discretizing (\ref{cont_rate_match_2_perturb3}) using the rate-matching method with $t_k=k\sqrt{\epsilon}$ gives\footnote{Here, we did not replace $\nabla f(x_k)$ in the third line with $\nabla f(y_k)$ after discretization (this is the case in \cite{WibisonoE7351}). Also, the same shifting trick as in \cite{WibisonoE7351} ($k\rightarrow k+2$) is done in the update of $x_{k}$.}
\begin{align}\label{rate_match_3}
    \left\{\begin{array}{l}
    x_{k+1}=\frac{2}{k+2}z_k+\frac{k}{k+2}y_k,\\
    y_{k}=x_k-\epsilon\nabla f(x_k),   \\
    z_{k}=z_{k-1} -\frac{\epsilon}{2} (k+2)\nabla f(x_{k})  ,
    \end{array}\right.
\end{align}
which is exactly the Nesterov's algorithm. Since (\ref{rate_match_3}) is exactly the NAG method, its high-resolution ODE is (\ref{HR_cnts_general_stable_ODE}). This means that the corresponding high-resolution ODE of (\ref{rate_match_3}) is 
\begin{align}\label{cont_rate_match_2_perturb4}
    \left\{\begin{array}{l}
         \dot X_t = \frac{2}{t}(Z_t-X_t)-\sqrt{\epsilon}\nabla f(X_t),  \\
          \dot Z_t = -\frac{t}{2}\nabla f(X_t)-\sqrt{\epsilon}\nabla f(X_t).
    \end{array}
    \right.
\end{align}
which is the perturbed version of (\ref{cont_rate_match_2_perturb3}).


\subsection{Stochastic Extentions}
In this section we will propose a stochastic variation of (\ref{new_algorithm_stochastic}). This method is a direct consequence of (\ref{new_algorithm}). We model noisy gradients by adding i.i.d noise $e_k$ to the gradients, 
\begin{align}\label{new_algorithm_stochastic}
   \left\{ \begin{array}{ll}
    &x_{k+1}   =    x_{k} + \frac{2s_k}{t_k}(v_k-x_{k+1})-\frac{s_k}{\sqrt{L}}(\nabla f(x_k)+e_k),\\
     &v_{k+1}    = v_k -\tfrac{1}{2}((t_k)s_k)(\nabla f(x_{k+1})+e_{k+1})-s_k^2(\nabla f(x_{k+1})+e_{k+1}). 
    \end{array}\right.
\end{align}
The updates (\ref{new_algorithm_stochastic}) is interesting due to its capability of dealing with perturbed gradients. This is the case in practical methods e.g. SGD \cite{bottou2010large}, SAG \cite{schmidt2017minimizing}, SAGA \cite{defazio2014saga}, SVRG \cite{johnson2013accelerating}, and etc.
In what follows, we prove the convergence of (\ref{new_algorithm_stochastic}) and show that (\ref{new_algorithm_stochastic}) decreases $\mathbb E\left[\min_{0\leq i\leq k-1}\|\nabla f(x_i)\|^2 \right]$ with a rate of $O(k^{(-3/4)})$ where $O$ hides a factor of $\log(k)$. The latter was missed in \cite{pmlr-v108-laborde20a} \hcm{check the literature and see if we can make a comparison with current rates here}
\begin{thm}\label{Theorem5}
    Consider $t_k=\sum_{i=1}^k s_k $ and $s_k=\frac{c}{k^{\alpha}}$ with $c\leq \frac{1}{\sqrt{L}}$ and $\frac{3}{4}\leq\alpha<1$. Then, if the update (\ref{new_algorithm_stochastic}) is used to find $x^*=\arg\min_x f(x)$, we will have
    \begin{align}\label{conv_rate1_them6}
    \mathbb E[f(x_k)]-f(x^*)&\leq \left\{\begin{array}{lr}
         \frac{\mathbb E[\varepsilon (0)]+\frac{c^4\sigma^2}{8}\left[16(1+\log(k))+32+6\right]}{2c^2\left[2(k^{\frac{1}{4}}-1)^2+k^{-\frac{3}{4}}(k^{\frac{1}{4}}-1)\right]} & \quad \alpha=\frac{3}{4} \\
          \frac{\mathbb E[\varepsilon (0)]+\frac{c^4\sigma^2}{8}\left[\frac{(4\alpha -2)}{(1-\alpha)^2(4\alpha-3)}+\frac{4(4\alpha -1)}{(1-\alpha)(4\alpha -2)}+\frac{4(4\alpha )}{(4\alpha -1)}\right]}{\frac{c^2}{2(1-\alpha)}\left[\frac{(k^{1-\alpha}-1)^2}{2(1-\alpha)}+k^{-\alpha}(k^{(1-\alpha)}-1)\right]}& \quad 1>\alpha>\frac{3}{4}
    \end{array}\right.,
\end{align}
with $\mathbb E[\varepsilon (0)]=\frac{1}{2}\|v_0-x^*\|^2$. In addition, for $\alpha = 3/4$ we have
\begin{align}\label{conv_rate2_them6}
    \mathbb E\left[\min_{0\leq i\leq k-1}\|\nabla f(x_i)\|^2 \right] &\leq \frac{2\sqrt{L}\mathbb E[\varepsilon (0)]}{16c^3\left( \frac{k^{3/4}-1}{3} +k^{1/4}-\frac{3}{2}+k^{1/2}\right)}\nonumber\\
    &+2c\sigma^2\sqrt{L}\frac{2\log (k)+6+\frac{3}{4}}{16\left( \frac{k^{3/4}-1}{3} +k^{1/4}-\frac{3}{2}+k^{1/2}\right)}.
\end{align}
\end{thm}
\begin{rem}
Note that $c\leq 1/\sqrt{L}$ and therefore, both rates in Theorem \ref{Theorem5} are independent of $L$. 
\end{rem}
\begin{proof}
    Take the Lyapunov function
    \begin{align}\label{Lyap_stochastic}
    \varepsilon(k)= (\frac{t_k^2}{4}+\frac{s_kt_k}{2})(f(x_k)-f(x^*))+\frac{1}{2}\|v_k-x^*\|^2.
    \end{align}
    Next, we will bound the difference $\varepsilon(k+1)-\varepsilon(k)$. Using (\ref{Lyap_stochastic}) we have
    \begin{align}\label{Lyap_stc_1}
        \varepsilon(k+1)-\varepsilon(k)&=(\frac{t_{k+1}^2}{4}+\frac{s_{k+1}t_{k+1}}{2})(f(x_{k+1})-f(x^*))\nonumber\\
        &-(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})(f(x_{k})-f(x^*))+\frac{1}{2}(\|v_{k+1}-x^*\|^2-\|v_{k}-x^*\|^2),\nonumber\\
        & = (\frac{t_{k+1}^2-t_k^2}{4}+\frac{s_{k+1}t_{k+1}-t_ks_k}{2})(f(x_{k+1})-f(x^*))\nonumber\\
        &+(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})(f(x_{k+1})-f(x_k))+\frac{1}{2}(\|v_{k+1}-x^*\|^2-\|v_{k}-x^*\|^2),\nonumber\\
        &= (\frac{t_{k+1}^2-t_k^2}{4}+\frac{s_{k+1}t_{k+1}-t_ks_k}{2})(f(x_{k+1})-f(x^*))\nonumber\\
        &+(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})(f(x_{k+1})-f(x_k))+\frac{1}{2}(\|v_{k+1}-v_k\|^2+2\langle v_{k+1}-v_k,v_k-x^*\rangle),
    \end{align}
    where in the last equality we used 
    $$\langle a-b,a-c\rangle = \frac{1}{2}(\|a-b\|^2+\|a-c\|^2-\|b-c\|^2).$$
    Next, from the update (\ref{new_algorithm_stochastic}) we have
    \begin{align}\label{Lyap_stc_2}
        \left\{\begin{array}{cl}
             v_k-x^*=&\frac{t_k}{2s_k}(x_{k+1}-x_k)+x_{k+1}-x^*+\frac{t_k}{2\sqrt{L}}(\nabla f(x_k)+e_k),  \\
            v_{k+1}-v_k=& -\frac{1}{2}(t_k+2s_k)s_k (\nabla f(x_{k+1})+e_{k+1}).
        \end{array}\right.
    \end{align}
    Using (\ref{Lyap_stc_2}) in (\ref{Lyap_stc_1}) we have
    \begin{align}\label{Lyap_stc_3}
        \varepsilon(k+1)-\varepsilon(k)&=(\frac{t_{k+1}^2-t_k^2}{4}+\frac{s_{k+1}t_{k+1}-t_ks_k}{2})(f(x_{k+1})-f(x^*))\nonumber\\
        &+(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})(f(x_{k+1})-f(x_k))+\frac{1}{8}((t_k+2s_k)s_k)^2\|\nabla f(x_{k+1})+e_{k+1}\|^2\nonumber\\
        &-\frac{1}{2}\langle (t_k+2s_k)s_k (\nabla f(x_{k+1})+e_{k+1}), \frac{t_k}{2s_k}(x_{k+1}-x_k)+x_{k+1}-x^*\nonumber\\
        &+\frac{t_k}{2\sqrt{L}}(\nabla f(x_k)+e_k)\rangle.
    \end{align}
    Now, using (\ref{smooth_convex}) in (\ref{Lyap_stc_3}) we get
    \begin{align}\label{Lyap_stc_4}
         \varepsilon(k+1)-\varepsilon(k)&\leq (\frac{t_{k+1}^2-t_k^2}{4}+\frac{s_{k+1}t_{k+1}-t_ks_k}{2})(\langle \nabla f(x_{k+1}),x_{k+1}-x^* \rangle-\frac{1}{2L}\|\nabla f(x_{k+1})\|^2)\nonumber\\
         & +(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})(\langle \nabla f(x_{k+1}),x_{k+1}-x_k \rangle-\frac{1}{2L}\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2)\nonumber\\
         & +\frac{1}{8}((t_k+2s_k)s_k)^2(\|\nabla f(x_{k+1})\|^2+\|e_{k+1}\|^2+2\langle \nabla f(x_{k+1}) ,e_k \rangle) \nonumber\\
         & -(\frac{t_k^2}{4}+\frac{s_kt_k}{2})\langle \nabla f(x_{k+1})+e_{k+1},x_{k+1}-x_k\rangle\nonumber\\
         &-\frac{(t_k+2s_k)s_k}{2}\langle \nabla f(x_{k+1})+e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         & -\frac{t_k(t_k+2s_k)s_k}{4\sqrt{L}}\langle \nabla f(x_{k+1})+e_{k+1}, \nabla f(x_k)+e_k \rangle.
         \end{align}
         Now, note that in (\ref{Lyap_stc_4}) the terms containing $\langle \nabla f(x_{k+1}),x_{k+1}-x_k \rangle$ disappear. Due to $t_k=\sum_{i=1}^k s_k$, we get $t_{k+1}^2=t_k^2 + 2t_ks_{k+1}+ s_{k+1}^2$ and $s_{k+1}t_{k+1}=s_{k+1}t_k+s_{k+1}^2$. Also, note  that by definition $s_{k+1}\leq s{k}$ as long as $0<\alpha<1$. Thus,
         $$\left(\frac{t_{k+1}^2-t_k^2}{4}+\frac{s_{k+1}t_{k+1}-t_ks_k}{2} -\frac{(t_k+2s_k)s_k}{2}\right)\leq 0.$$
         Then, due to convexity and smoothness of $f$ we get
         \begin{align}\label{Lyap_stc_5}
             \left(\frac{t_{k+1}^2-t_k^2}{4}\right.&\left.+\frac{s_{k+1}t_{k+1}-t_ks_k}{2} -\frac{(t_k+2s_k)s_k}{2}\right)\langle \nabla f(x_{k+1}), x_{k+1}-x^* \rangle\nonumber\\
             &\leq \frac{\left(\frac{t_{k+1}^2-t_k^2}{4}+\frac{s_{k+1}t_{k+1}-t_ks_k}{2} -\frac{(t_k+2s_k)s_k}{2}\right)}{2L}\|\nabla f(x_{k+1})\|^2.
         \end{align} 
        Replacing (\ref{Lyap_stc_5}) in (\ref{Lyap_stc_4}) and simplification gives
        \begin{align}\label{Lyap_stc_6}
            \varepsilon(k+1)-\varepsilon(k)&\leq -\frac{(t_k+2s_k)s_k}{4L} \|\nabla f(x_{k+1})\|^2-\frac{(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})}{2L}\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2\nonumber\\
            &+\frac{1}{8}((t_k+2s_k)s_k)^2(\|\nabla f(x_{k+1})\|^2+\|e_{k+1}\|^2+2\langle \nabla f(x_{k+1}) ,e_k \rangle) \nonumber\\
            & -(\frac{t_k^2}{4}+\frac{s_kt_k}{2})\langle e_{k+1},x_{k+1}-x_k\rangle-\frac{(t_k+2s_k)s_k}{2}\langle e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         & -\frac{t_k(t_k+2s_k)s_k}{4\sqrt{L}}\langle \nabla f(x_{k+1})+e_{k+1}, \nabla f(x_k)+e_k \rangle,\nonumber\\
         &=\frac{1}{2}\left(\frac{t_k}{2}+s_k\right)^2(s_k^2-\frac{1}{L})\|\nabla f(x_{k+1})\|^2\nonumber\\
         &+\frac{1}{2}\left(\frac{t_k^2}{4}+\frac{s_kt_k}{2} \right)(\frac{1}{L}-\frac{s_k}{\sqrt{L}}) 2\langle \nabla f(x_{k+1}),\nabla f(x_k) \rangle\nonumber\\
         &-\frac{(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})}{2L}\|\nabla f(x_k)\|^2+\frac{1}{8}((t_k+2s_k)s_k)^2(\|e_{k+1}\|^2+2\langle \nabla f(x_{k+1}) ,e_k \rangle)\nonumber\\
         & -(\frac{t_k^2}{4}+\frac{s_kt_k}{2})\langle e_{k+1},x_{k+1}-x_k\rangle-\frac{(t_k+2s_k)s_k}{2}\langle e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         &-\frac{t_k(t_k+2s_k)s_k}{4\sqrt{L}}\left(\langle \nabla f(x_{k+1}),e_k \rangle+\langle \nabla f(x_{k}) , e_{k+1}\rangle+\langle e_{k+1},e_k\rangle\right).
        \end{align}
        Here, the proof divides in 2 sections for each of the results in Theorem \ref{Theorem5}. First, we prove the rate for $\mathbb E\left[f(x_k)\right]-f(x^*)$. 
        
        a) Taking $c\leq 1/\sqrt{L}$ in $s_k=c/k^{\alpha}$, we get $s_k\leq 1/\sqrt{L}$. This implies 
        \begin{align}\label{Lyap_stc_7}
            -\frac{1}{2}\left(\frac{t_k}{2}+s_k\right)^2(\frac{1}{L}-s_k^2)&\leq -\frac{1}{2}\left(\frac{t_k^2}{4}+\frac{s_kt_k}{2} \right)(\frac{1}{L}-\frac{s_k}{\sqrt{L}}),\nonumber\\
            -\frac{1}{2L}\left(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2}\right)&\leq -\frac{1}{2}\left(\frac{t_k^2}{4}+\frac{s_kt_k}{2} \right)(\frac{1}{L}-\frac{s_k}{\sqrt{L}}).
        \end{align}
        Utilizing (\ref{Lyap_stc_7}) in (\ref{Lyap_stc_6}) gives
        \begin{align}\label{Lyap_stc_8}
            \varepsilon(k+1)-\varepsilon(k)&\leq -\frac{1}{2}\left(\frac{t_k^2}{4}+\frac{s_kt_k}{2} \right)(\frac{1}{L}-\frac{s_k}{\sqrt{L}})\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2\nonumber\\
            &\frac{1}{8}((t_k+2s_k)s_k)^2(\|e_{k+1}\|^2)+\frac{1}{4}((t_k+2s_k)s_k)^2\langle \nabla f(x_{k+1}) ,e_k \rangle)\nonumber\\
         & -(\frac{t_k^2}{4}+\frac{s_kt_k}{2})\langle e_{k+1},x_{k+1}-x_k\rangle-\frac{(t_k+2s_k)s_k}{2}\langle e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         &-\frac{t_k(t_k+2s_k)s_k}{4\sqrt{L}}\left(\langle \nabla f(x_{k+1}),e_k \rangle+\langle \nabla f(x_{k}) , e_{k+1}\rangle+\langle e_{k+1},e_k\rangle\right),\nonumber\\
         &\leq \frac{1}{8}((t_k+2s_k)s_k)^2\|e_{k+1}\|^2+\frac{1}{4}((t_k+2s_k)s_k)^2\langle \nabla f(x_{k+1}) ,e_k \rangle)\nonumber\\
         & -(\frac{t_k^2}{4}+\frac{s_kt_k}{2})\langle e_{k+1},x_{k+1}-x_k\rangle-\frac{(t_k+2s_k)s_k}{2}\langle e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         &-\frac{t_k(t_k+2s_k)s_k}{4\sqrt{L}}\left(\langle \nabla f(x_{k+1}),e_k \rangle+\langle \nabla f(x_{k}) , e_{k+1}\rangle+\langle e_{k+1},e_k\rangle\right),\nonumber\\
         &\leq \frac{s_k(t_k+2s_k)}{2}\left(\frac{(t_k+2s_k)s_k}{4}\|e_{k+1}\|^2+\frac{s_k(t_k+2s_k)}{2}+\langle \nabla f(x_{k+1}) ,e_k \rangle\right.\nonumber\\
         &-\frac{t_k}{s_k}\langle e_{k+1},x_{k+1}-x_k\rangle-\langle e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         &\left.-\frac{t_k}{2\sqrt{L}}\left(\langle \nabla f(x_{k+1}),e_k \rangle+\langle \nabla f(x_{k}) , e_{k+1}\rangle+\langle e_{k+1},e_k\rangle\right)\right)\nonumber\\
         &=\frac{s_k(t_k+2s_k)}{2}g_k.
        \end{align}
        Next, note that $\mathbb E[g_k]= \frac{s_k(t_k+2s_k)}{4}\sigma^2$ and therefore, $\mathbb E [\varepsilon(k+1)]-\mathbb E[ \varepsilon (k)]\leq \frac{s_k^2(t_k+2s_k)^2}{8}\sigma^2$. Also, by the form of $\varepsilon(k)$ in (\ref{Lyap_stochastic}) we have $$(\frac{t_k^2}{4}+\frac{t_{k}s_k}{2})(\mathbb E[f(x_k)]-f(x^*))\leq \mathbb E[\varepsilon(k)].$$
        Thus, forming a telescope summation leads to
        \begin{align}\label{Lyap_stc_9}
            (\frac{t_k^2}{4}+\frac{t_{k}s_k}{2})(\mathbb E[f(x_k)]-f(x^*))\leq\mathbb E[\varepsilon(k)]\leq \mathbb E[\varepsilon(0)] + \sum_{i=1}^{k-1} \frac{s_i^2(t_i+2s_i)^2}{8}\sigma^2,
        \end{align}
        with $s_0=t_0=0$. From (\ref{Lyap_stc_9}) one can get
        \begin{align}\label{Lyap_stc_10}
            \mathbb E[f(x_k)]-f(x^*) \leq \frac{\mathbb E[\varepsilon(0)] + \sum_{i=1}^{k-1} \frac{s_i^2(t_i+2s_i)^2}{8}\sigma^2}{(\frac{t_k^2}{4}+\frac{t_{k}s_k}{2})}.
        \end{align}
        Now, we should bound $\sum_{i=1}^{k-1} s_i^2(t_i+2s_i)^2$. Note that 
        $$\sum_{i=1}^{k-1} s_i^2(t_i+2s_i)^2=\sum_{i=1}^{k-1} s_i^2t_i^2+4t_is_i^3+4s_i^4,$$
        and
        $$t_i=\left(\sum_{j=1}^i\frac{c}{j^{\alpha}}\right)\leq \left(\int_{0}^{i} \frac{c}{t^{\alpha}}dt\right)=\frac{ci^{1-\alpha}}{(1-\alpha)},$$
        $$t_i^2=\left(\sum_{j=1}^i\frac{c}{j^{\alpha}}\right)^2\leq \left(\int_{0}^{i} \frac{c}{t^{\alpha}}dt\right)^2=\frac{c^2i^{2-2\alpha}}{(1-\alpha)^2}.$$
        Therefore, for the first term we have 
        \begin{align}\label{Lyap_stc_11}
            \sum_{i=1}^{k-1} s_i^2t_i^2&\leq \frac{c^4}{(1-\alpha)^2}\sum_{i=1}^{k-1} \frac{1}{i^{4\alpha -2}}\leq \frac{c^4}{(1-\alpha)^2}(1+\int_{1}^{k} \frac{1}{t^{4\alpha -2}}dt)\leq \frac{c^4}{(1-\alpha)^2}(1+\frac{1}{4\alpha-3})\nonumber\\
            &=\frac{c^4(4\alpha -2)}{(1-\alpha)^2(4\alpha-3)},
        \end{align}
        when $\alpha > 3/4$ and if $\alpha=3/4$ we have
        \begin{align}\label{Lyap_stc_12}
            \sum_{i=1}^{k-1} s_i^2t_i^2&\leq 16c^4\sum_{i=1}^{k-1}\frac{1}{i}\leq 16c^4(1+\log (k)).
        \end{align}
        For the second term we have
        \begin{align}\label{Lyap_stc_13}
            \sum_{i=1}^{k-1} s_i^3t_i&\leq \frac{c^4}{(1-\alpha)}\sum_{i=1}^{k-1} \frac{1}{i^{4\alpha -1}}\leq \frac{c^4}{(1-\alpha)}(1+\int_1^k\frac{1}{t^{4\alpha -1}}dt)\leq \frac{c^4(4\alpha -1)}{(1-\alpha)(4\alpha -2)}
         \end{align}
         for $\alpha > 1/2$. The third term gives
        \begin{align}\label{Lyap_stc_14}
            \sum_{i=1}^{k-1} s_i^4&\leq c^4\sum_{i=1}^{k-1} \frac{1}{i^{4\alpha }}\leq c^4(1+\int_1^k\frac{1}{t^{4\alpha }}dt)\leq \frac{c^4(4\alpha )}{(4\alpha -1)},
         \end{align} 
         for $\alpha > 1/4$.\par
         For the terms in denominator of (\ref{Lyap_stc_10}) we use lower bounds as
         \begin{align}\label{Lyap_stc_15}
             \frac{t_k^2}{4}\geq \frac{c^2}{4(1-\alpha)^2}(k^{1-\alpha}-1)^2,\nonumber\\
             \frac{t_ks_k}{2}\geq \frac{c^2k^{-\alpha}}{2(1-\alpha)}(k^{1-\alpha}-1).
         \end{align}
Using(\ref{Lyap_stc_11},\ref{Lyap_stc_12},\ref{Lyap_stc_13},\ref{Lyap_stc_14},\ref{Lyap_stc_15}) in (\ref{Lyap_stc_10}) leads to
\begin{align}\label{Lyap_stc_16}
    \mathbb E[f(x_k)]-f(x^*)\leq \left\{\begin{array}{lr}
         \frac{\mathbb E[\varepsilon (0)]+\frac{c^4\sigma^2}{8}\left[16(1+\log(k))+32+6\right]}{2c^2\left[2(k^{\frac{1}{4}}-1)^2+k^{-\frac{3}{4}}(k^{\frac{1}{4}}-1)\right]} & \quad \alpha=\frac{3}{4} \\
          \frac{\mathbb E[\varepsilon (0)]+\frac{c^4\sigma^2}{8}\left[\frac{(4\alpha -2)}{(1-\alpha)^2(4\alpha-3)}+\frac{4(4\alpha -1)}{(1-\alpha)(4\alpha -2)}+\frac{4(4\alpha )}{(4\alpha -1)}\right]}{\frac{c^2}{2(1-\alpha)}\left[\frac{(k^{1-\alpha}-1)^2}{2(1-\alpha)}+k^{-\alpha}(k^{(1-\alpha)}-1)\right]}& \quad 1>\alpha>\frac{3}{4}
    \end{array}\right.
\end{align}
with $\mathbb E[\varepsilon (0)]=\frac{1}{2}\|v_0-x^*\|^2$. 


\hcm{for stochastic gradient norm minimization show that the stochastic extention of other guys (chen and shi) proof may not be good.}

b) On the transition from (\ref{Lyap_stc_6}) to (\ref{Lyap_stc_8}), one can write
\begin{align}\label{stc_GNM1}
    \varepsilon(k+1)-\varepsilon(k)&\leq \frac{s_k(t_k+2s_k)}{2}g_k-\frac{1}{2}\left( \frac{t_k^2}{4}+\frac{s_kt_k}{2} \right)(\frac{1}{L}-\frac{s_k}{\sqrt{L}})\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2\nonumber\\
    &-\frac{1}{2}\left(\frac{t_k^2}{4}+\frac{s_kt_k}{2}\right)(\frac{s_k}{\sqrt{L}})\|\nabla f(x_k)\|^2\nonumber\\
    &\leq \frac{s_k(t_k+2s_k)}{2}g_k-\frac{1}{2}\left(\frac{t_k^2}{4}+\frac{s_kt_k}{2}\right)(\frac{s_k}{\sqrt{L}})\|\nabla f(x_k)\|^2
\end{align}
Recursively summing (\ref{stc_GNM1}) from $0$ to $k$ gives
\begin{align}\label{stc_GNM2}
    0\leq\varepsilon(k)\leq \varepsilon(0)+ \sum_{i=0}^{k-1} \frac{s_i(t_i+2s_i)}{2}g_i -\frac{1}{2}\sum_{i=0}^{k-1}\left(\frac{t_i^2}{4}+\frac{s_it_i}{2}\right)(\frac{s_i}{\sqrt{L}})\|\nabla f(x_i)\|^2,
\end{align}
which results in
\begin{align}\label{stc_GNM3}
    \frac{1}{2}\sum_{i=0}^{k-1}\left(\frac{t_i^2}{4}+\frac{s_it_i}{2}\right)(\frac{s_i}{\sqrt{L}})\|\nabla f(x_i)\|^2\leq \varepsilon(0)+ \sum_{i=0}^{k-1} \frac{s_i(t_i+2s_i)}{2}g_i,
\end{align}
and therefore,
\begin{align}\label{stc_GNM4}
    \min_{0\leq i\leq k-1}\|\nabla f(x_i)\|^2 \leq \frac{\varepsilon(0)+ \sum_{i=0}^{k-1} \frac{s_i(t_i+2s_i)}{2}g_i}{\frac{1}{2\sqrt{L}}\sum_{i=0}^{k-1}\left(\frac{t_i^2}{4}+\frac{s_it_i}{2}\right)s_i},
\end{align}
Evaluating the expectation of both hand sides gives
\begin{align}\label{stc_GNM4}
    \mathbb E\left[\min_{0\leq i\leq k-1}\|\nabla f(x_i)\|^2 \right]&\leq \frac{\mathbb E\left[\varepsilon(0)\right]+ \sum_{i=0}^{k-1} \frac{s_i(t_i+2s_i)}{2}\mathbb E\left[g_i\right]}{\frac{1}{2\sqrt{L}}\sum_{i=0}^{k-1}\left(\frac{t_i^2}{4}+\frac{s_it_i}{2}\right)s_i},\nonumber\\
    &= \frac{\mathbb E\left[\varepsilon(0)\right]+ \sum_{i=0}^{k-1} \frac{s_i^2(t_i+2s_i)^2}{8}\sigma^2}{\frac{1}{2\sqrt{L}}\sum_{i=0}^{k-1}\left(\frac{t_i^2}{4}+\frac{s_it_i}{2}\right)s_i},
\end{align}
and the last equality is due to $\mathbb E[g_i]= \frac{s_i(t_i+2s_i)}{4}\sigma^2$. Next, we will bound the numerator from above and lower bound the denominator as 
\begin{align}\label{stc_GNM5}
    \sum_{i=0}^{k-1} \frac{s_i^2(t_i+2s_i)^2}{8}\sigma^2\overset{(\ref{Lyap_stc_12},\ref{Lyap_stc_13},\ref{Lyap_stc_14})}{\leq}2\sigma^2c^4(1+\log (k))+\frac{\sigma^2c^4(4\alpha -1)}{2(1-\alpha)(4\alpha -2)}+\frac{\sigma^2c^4(4\alpha )}{2(4\alpha -1)}
\end{align}
for $\alpha=3/4$. For bounding the denominator one can use (\ref{Lyap_stc_15}) as
        \begin{align}\label{stc_GNM6}
             \frac{s_it_i^2}{4}\geq \frac{c^3i^{-\alpha}}{4(1-\alpha)^2}(i^{1-\alpha}-1)^2,\nonumber\\
             \frac{t_is_i^2}{2}\geq \frac{c^3i^{-2\alpha}}{2(1-\alpha)}(i^{1-\alpha}-1).
         \end{align}
Applying the summation to (\ref{stc_GNM6}) gives
\begin{align}\label{stc_GNM7}
    \sum_{i=0}^{k-1}\left(\frac{s_it_i^2}{4}+\frac{s_i^2t_i}{2}\right)&\geq  \sum_{i=0}^{k-1} \frac{c^3}{4(1-\alpha)^2}(i^{2-3\alpha}-2i^{1-2\alpha}+i^{-\alpha})\nonumber\\
    & + \sum_{i=0}^{k-1} \frac{c^3}{2(1-\alpha)}(i^{1-3\alpha}-i^{-2\alpha})\nonumber\\
    &\geq \frac{c^3}{4(1-\alpha)^2}\left( \frac{k^{3(1-\alpha)}-1}{3-3\alpha} +\frac{(k^{1-\alpha}-1)}{1-\alpha}+\frac{1-2\alpha+k^{2-2\alpha}}{1-\alpha}\right)\nonumber\\
    &+\frac{c^3}{2(1-\alpha)}\left( \frac{k^{2-3\alpha}-1}{2-3\alpha} +\frac{(2\alpha-k^{1-2\alpha)}}{1-2\alpha}\right)\nonumber\\
    &\overset{\alpha=\tfrac{3}{4}}{=} 4c^3\left( \frac{k^{3/4}-1}{3/4} +\frac{(k^{1/4}-1)}{1/4}+\frac{-\tfrac{1}{2}+k^{1/2}}{1/4}\right)\nonumber\\
    &+2c^3\left( -4k^{-1/4}+4 -3+2k^{-1/2}\right)\nonumber\\
    &\geq 4c^3\left( \frac{k^{3/4}-1}{3/4} +\frac{(k^{1/4}-1)}{1/4}+\frac{-\tfrac{1}{2}+k^{1/2}}{1/4}\right)
\end{align}
Combining (\ref{stc_GNM7}) and (\ref{stc_GNM5}) gives
\begin{align}\label{stc_GNM8}
  2\sqrt{L}  \frac{ \sum_{i=0}^{k-1} \frac{s_i^2(t_i+2s_i)^2}{8}\sigma^2}{\sum_{i=0}^{k-1}\left(\frac{s_it_i^2}{4}+\frac{s_i^2t_i}{2}\right)}&\leq 2\sqrt{L}\frac{2\sigma^2c^4(1+\log (k))+\frac{\sigma^2c^4(4\alpha -1)}{2(1-\alpha)(4\alpha -2)}+\frac{\sigma^2c^4(4\alpha )}{2(4\alpha -1)}}{4c^3\left( \frac{k^{3/4}-1}{3/4} +\frac{(k^{1/4}-1)}{1/4}+\frac{-\tfrac{1}{2}+k^{1/2}}{1/4}\right)}\nonumber\\
  &\overset{\alpha=\tfrac{3}{4}}{=} 2c\sigma^2\sqrt{L}\frac{2\log (k)+6+\frac{3}{4}}{16\left( \frac{k^{3/4}-1}{3} +k^{1/4}-\frac{3}{2}+k^{1/2}\right)},
\end{align}
and therefore,
\begin{align}
    \mathbb E\left[\min_{0\leq i\leq k-1}\|\nabla f(x_i)\|^2 \right] &\leq \frac{2\sqrt{L}\mathbb E[\varepsilon (0)]}{16c^3\left( \frac{k^{3/4}-1}{3} +k^{1/4}-\frac{3}{2}+k^{1/2}\right)}\nonumber\\
    &+2c\sigma^2\sqrt{L}\frac{2\log (k)+6+\frac{3}{4}}{16\left( \frac{k^{3/4}-1}{3} +k^{1/4}-\frac{3}{2}+k^{1/2}\right)},
\end{align}
with $\mathbb E[\varepsilon(0)]=\tfrac{1}{2}\|x_0-x^*\|^2$.
\end{proof}
Note that when the noise is omitted ($\sigma=0$), the parameter $\alpha$ can become zero. This is because we do not have to alleviate the effect of noise with decreasing step-sizes. Therefore, we recover the convergence rate of $O(1/k^2)$ for the NAG method for $c=1/\sqrt{L}$.
Note that the analysis in \cite{pmlr-v108-laborde20a} cannot achieve the bound proposed in Theorem ? (see \cite{pmlr-v108-laborde20a} Appendix C.4)\par
If we know the smoothness coefficient $L$, the convergence rate (\ref{conv_rate1_them6}) can be improved. Consider the update
\begin{align}\label{new_alg_mixed_step}
    \left\{ \begin{array}{ll}
    &x_{k+1}   =    x_{k} + \frac{2s_k}{t_k}(v_k-x_{k+1})-\frac{\beta s_k}{\sqrt{L}}(\nabla f(x_k)+e_k),\\
     &v_{k+1}    = v_k -\tfrac{1}{2}(t_ks_k+\tfrac{2s_k \beta}{\sqrt{L}})(\nabla f(x_{k+1})+e_{k+1})
    \end{array}\right.
\end{align}
with $\beta = \tfrac{-2c^2+3}{4c^2}$. Then, the following convergence result holds.
\begin{thm}\label{Theorem6}
    Under the conditions of Theorem \ref{Theorem5} and $\beta = \tfrac{-2c^2+3}{4c^2}$, the update (\ref{new_alg_mixed_step}) will satisfy 
    \begin{align}\label{conv_rate1_them6}
    \mathbb E[f(x_k)]-f(x^*)&\leq \left\{\begin{array}{lr}
         \frac{\mathbb E[\varepsilon (0)]+\frac{\sigma^2}{8}\left[16(1+\log(k))+\frac{\beta(80c^3)}{\sqrt{L}}+\frac{24\beta^2c^2}{L}\right]}{\frac{c^2}{4}\left((k^{\frac{1}{4}}-1)^2\right)+\frac{2c\beta}{\sqrt{L}}\left((k^{\frac{1}{4}}-1)\right)} & \quad \alpha=\frac{3}{4} \\
          \frac{\mathbb E[\varepsilon (0)]+\frac{\sigma^2}{8}\left[\frac{c^4(4\alpha -2)}{(1-\alpha)^2(4\alpha-3)}+\frac{4\beta c^3(3\alpha -1)}{\sqrt{L}(1-\alpha)(3\alpha -2)}+\frac{4\beta^2c^2(42\alpha )}{L(2\alpha -1)}\right]}{\frac{c^2}{4(1-\alpha)^2}\left((k^{1-\alpha}-1)^2\right)+\frac{c\beta}{2\sqrt{L}(1-\alpha)}\left(k^{(1-\alpha)}-1\right)}& \quad 1>\alpha>\frac{3}{4}
    \end{array}\right.,
\end{align}
\end{thm}
\begin{proof}
 Take the Lyapunov function
    \begin{align}\label{Lyap_stochastic2}
    \varepsilon(k)= (\frac{t_k^2}{4}+\frac{t_k\beta}{2\sqrt{L}})(f(x_k)-f(x^*))+\frac{1}{2}\|v_k-x^*\|^2.
    \end{align}
     Using (\ref{Lyap_stochastic2}) we have
    \begin{align}\label{Lyap2_stc_1}
        \varepsilon(k+1)-\varepsilon(k)&=(\frac{t_{k+1}^2}{4}+\frac{\beta t_{k+1}}{2\sqrt{L}})(f(x_{k+1})-f(x^*))\nonumber\\
        &-(\frac{t_{k}^2}{4}+\frac{\beta t_{k}}{2\sqrt{L}})(f(x_{k})-f(x^*))+\frac{1}{2}(\|v_{k+1}-x^*\|^2-\|v_{k}-x^*\|^2),\nonumber\\
        & = (\frac{t_{k+1}^2-t_k^2}{4}+\frac{\beta t_{k+1}-\beta t_k}{2\sqrt{L}})(f(x_{k+1})-f(x^*))\nonumber\\
        &+(\frac{t_{k}^2}{4}+\frac{\beta t_{k}}{2\sqrt{L}})(f(x_{k+1})-f(x_k))+\frac{1}{2}(\|v_{k+1}-x^*\|^2-\|v_{k}-x^*\|^2),\nonumber\\
        &= (\frac{t_{k+1}^2-t_k^2}{4}+\frac{\beta(t_{k+1}-t_k)}{2\sqrt{L}})(f(x_{k+1})-f(x^*))\nonumber\\
        &+(\frac{t_{k}^2}{4}+\frac{\beta t_{k}}{2\sqrt{L}})(f(x_{k+1})-f(x_k))+\frac{1}{2}(\|v_{k+1}-v_k\|^2+2\langle v_{k+1}-v_k,v_k-x^*\rangle),
    \end{align}
    where in the last equality we used 
    $$\langle a-b,a-c\rangle = \frac{1}{2}(\|a-b\|^2+\|a-c\|^2-\|b-c\|^2).$$
    Next, from the update (\ref{new_alg_mixed_step}) we have
    \begin{align}\label{Lyap2_stc_2}
        \left\{\begin{array}{cl}
             v_k-x^*=&\frac{t_k}{2s_k}(x_{k+1}-x_k)+x_{k+1}-x^*+\frac{t_k\beta }{2\sqrt{L}}(\nabla f(x_k)+e_k),  \\
            v_{k+1}-v_k=& -\frac{1}{2}(t_ks_k+\tfrac{2s_k\beta}{\sqrt{L}}) (\nabla f(x_{k+1})+e_{k+1}).
        \end{array}\right.
    \end{align}
    Using (\ref{Lyap2_stc_2}) in (\ref{Lyap2_stc_1}) we have
    \begin{align}\label{Lyap_stc_3}
        \varepsilon(k+1)-\varepsilon(k)&=(\frac{t_{k+1}^2-t_k^2}{4}+\frac{\beta(t_{k+1}-t_k)}{2\sqrt{L}})(f(x_{k+1})-f(x^*))\nonumber\\
        &+(\frac{t_{k}^2}{4}+\frac{\beta t_{k}}{2\sqrt{L}})(f(x_{k+1})-f(x_k))+\frac{1}{8}((t_k+\frac{2\beta}{\sqrt{L}})s_k)^2\|\nabla f(x_{k+1})+e_{k+1}\|^2\nonumber\\
        &-\frac{1}{2}\langle (t_k+\frac{2\beta}{\sqrt{L}})s_k (\nabla f(x_{k+1})+e_{k+1}), \frac{t_k}{2s_k}(x_{k+1}-x_k)+x_{k+1}-x^*\nonumber\\
        &+\frac{t_k\beta}{2\sqrt{L}}(\nabla f(x_k)+e_k)\rangle.
    \end{align}
    Now, using (\ref{smooth_convex}) in (\ref{Lyap_stc_3}) we get
    \begin{align}\label{Lyap2_stc_4}
         \varepsilon(k+1)-\varepsilon(k)&\leq (\frac{t_{k+1}^2-t_k^2}{4}+\frac{\beta(t_{k+1}-t_k)}{2\sqrt{L}})(\langle \nabla f(x_{k+1}),x_{k+1}-x^* \rangle-\frac{1}{2L}\|\nabla f(x_{k+1})\|^2)\nonumber\\
         & +(\frac{t_{k}^2}{4}+\frac{\beta t_{k}}{2\sqrt{L}})(\langle \nabla f(x_{k+1}),x_{k+1}-x_k \rangle-\frac{1}{2L}\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2)\nonumber\\
         & +\frac{1}{8}((t_k+\frac{2\beta}{2\sqrt{L}})s_k)^2(\|\nabla f(x_{k+1})\|^2+\|e_{k+1}\|^2+2\langle \nabla f(x_{k+1}) ,e_k \rangle) \nonumber\\
         & -(\frac{t_k^2}{4}+\frac{\beta t_k}{2\sqrt{L}})\langle \nabla f(x_{k+1})+e_{k+1},x_{k+1}-x_k\rangle\nonumber\\
         &-\frac{(t_k+\tfrac{2\beta}{\sqrt{L}})s_k}{2}\langle \nabla f(x_{k+1})+e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         & -\frac{\beta t_k(t_k+\frac{2\beta}{\sqrt{L}})s_k}{4\sqrt{L}}\langle \nabla f(x_{k+1})+e_{k+1}, \nabla f(x_k)+e_k \rangle.
         \end{align}
          Due to $t_k=\sum_{i=1}^k s_k$, we get $t_{k+1}^2=t_k^2 + 2t_ks_{k+1}+ s_{k+1}^2$ and $s_{k+1}t_{k+1}=s_{k+1}t_k+s_{k+1}^2$. Also, note  that by definition $s_{k+1}\leq s{k}$ as long as $0<\alpha<1$. Thus,
         $$\left(\frac{t_{k+1}^2-t_k^2}{4}+\frac{s_{k+1}t_{k+1}-t_ks_k}{2} -\frac{(t_k+2s_k)s_k}{2}\right)\leq 0.$$
         Then, due to convexity and smoothness of $f$ we get
         \begin{align}\label{Lyap_stc_5}
             \left(\frac{t_{k+1}^2-t_k^2}{4}\right.&\left.+\frac{s_{k+1}t_{k+1}-t_ks_k}{2} -\frac{(t_k+2s_k)s_k}{2}\right)\langle \nabla f(x_{k+1}), x_{k+1}-x^* \rangle\nonumber\\
             &\leq \frac{\left(\frac{t_{k+1}^2-t_k^2}{4}+\frac{s_{k+1}t_{k+1}-t_ks_k}{2} -\frac{(t_k+2s_k)s_k}{2}\right)}{2L}\|\nabla f(x_{k+1})\|^2.
         \end{align} 
        Replacing (\ref{Lyap_stc_5}) in (\ref{Lyap_stc_4}) and simplification gives
        \begin{align}\label{Lyap_stc_6}
            \varepsilon(k+1)-\varepsilon(k)&\leq -\frac{(t_k+2s_k)s_k}{4L} \|\nabla f(x_{k+1})\|^2-\frac{(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})}{2L}\|\nabla f(x_{k+1})-\nabla f(x_k)\|^2\nonumber\\
            &+\frac{1}{8}((t_k+2s_k)s_k)^2(\|\nabla f(x_{k+1})\|^2+\|e_{k+1}\|^2+2\langle \nabla f(x_{k+1}) ,e_k \rangle) \nonumber\\
            & -(\frac{t_k^2}{4}+\frac{s_kt_k}{2})\langle e_{k+1},x_{k+1}-x_k\rangle-\frac{(t_k+2s_k)s_k}{2}\langle e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         & -\frac{t_k(t_k+2s_k)s_k}{4\sqrt{L}}\langle \nabla f(x_{k+1})+e_{k+1}, \nabla f(x_k)+e_k \rangle,\nonumber\\
         &=\frac{1}{2}\left(\frac{t_k}{2}+s_k\right)^2(s_k^2-\frac{1}{L})\|\nabla f(x_{k+1})\|^2\nonumber\\
         &+\frac{1}{2}\left(\frac{t_k^2}{4}+\frac{s_kt_k}{2} \right)(\frac{1}{L}-\frac{s_k}{\sqrt{L}}) 2\langle \nabla f(x_{k+1}),\nabla f(x_k) \rangle\nonumber\\
         &-\frac{(\frac{t_{k}^2}{4}+\frac{s_{k}t_{k}}{2})}{2L}\|\nabla f(x_k)\|^2+\frac{1}{8}((t_k+2s_k)s_k)^2(\|e_{k+1}\|^2+2\langle \nabla f(x_{k+1}) ,e_k \rangle)\nonumber\\
         & -(\frac{t_k^2}{4}+\frac{s_kt_k}{2})\langle e_{k+1},x_{k+1}-x_k\rangle-\frac{(t_k+2s_k)s_k}{2}\langle e_{k+1},x_{k+1}-x^*\rangle\nonumber\\
         &-\frac{t_k(t_k+2s_k)s_k}{4\sqrt{L}}\left(\langle \nabla f(x_{k+1}),e_k \rangle+\langle \nabla f(x_{k}) , e_{k+1}\rangle+\langle e_{k+1},e_k\rangle\right).
        \end{align}

    
\end{proof}






\section{Numerical Results}\label{sec_numerical}
In this section we will investigate the performance of (\ref{new_algorithm_stochastic}) for various 
\section{Conclusion}\label{sec_conclusion}
\bibliographystyle{plainnat}
\bibliography{references}



\end{document}
